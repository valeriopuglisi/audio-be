{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_learning_features_audio import *\n",
    "from deep_learning_dict_api import AudioAnalysisAPI\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from torchmetrics import ScaleInvariantSignalNoiseRatio, ScaleInvariantSignalDistortionRatio, SignalNoiseRatio, SignalDistortionRatio, PermutationInvariantTraining\n",
    "from torchmetrics.audio.pesq import PerceptualEvaluationSpeechQuality\n",
    "from torchmetrics.functional.audio import signal_distortion_ratio\n",
    "from torchmetrics.audio.stoi import ShortTimeObjectiveIntelligibility\n",
    "from datetime import datetime\n",
    "from deep_learning_dict_datasets import Datasets\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cu102\n",
      "0.11.0+cu102\n"
     ]
    }
   ],
   "source": [
    "# When running this tutorial in Google Colab, install the required packages\n",
    "# with the following.\n",
    "# !pip install torchaudio librosa boto3\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as TAF\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================>  Dataset: Libri3Mix16kMax\n",
      "Datasets[task][dataset] : /storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/metadata/mixture_test_mix_both.csv\n",
      "len(test_table): 3000\n",
      "====> Benchmarking: 1/3   tot 3000\n",
      "model_sample_rate:8000 - dataset_sample_rate:16000\n",
      "revisited mixture_path:/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/mix_both/resampled_khz160004077-13754-0001_5142-33396-0065_5683-32866-0012.wav\n",
      "revisited source_1_path:/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s1/resampled_khz160004077-13754-0001_5142-33396-0065_5683-32866-0012.wav\n",
      "revisited source_2_path:/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s2/resampled_khz160004077-13754-0001_5142-33396-0065_5683-32866-0012.wav\n",
      "revisited source_3_path:/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s3/resampled_khz160004077-13754-0001_5142-33396-0065_5683-32866-0012.wav\n",
      "source_1_path : /storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s1/resampled_khz160004077-13754-0001_5142-33396-0065_5683-32866-0012.wav\n",
      "source_1_sample_rate:16000\n",
      "source_1_waveform.shape:torch.Size([1, 66080])\n",
      "source_1_waveform:tensor([-0.0114, -0.0171, -0.0155,  ...,  0.0044,  0.0043,  0.0128])\n",
      "\n",
      "source_1_path_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source1_resampled_khz160004077-13754-0001_5142-33396-0065_5683-32866-0012.wav\n",
      "source_1_prediction_sample_rate: 8000\n",
      "source_1_prediction_waveform.shape : torch.Size([1, 66080])\n",
      "source_1_prediction_waveform: tensor([ 0.0232,  0.0279,  0.0239,  ..., -0.0012, -0.0010, -0.0008])\n",
      "\n",
      "source_2_path : /storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s2/resampled_khz160004077-13754-0001_5142-33396-0065_5683-32866-0012.wav\n",
      "source_2_sample_rate:16000\n",
      "source_2_waveform.shape:torch.Size([1, 66080])\n",
      "source_2_waveform:tensor([-0.0107, -0.0163, -0.0144,  ...,  0.0044,  0.0043,  0.0128])\n",
      "\n",
      "source_2_path_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source2_resampled_khz160004077-13754-0001_5142-33396-0065_5683-32866-0012.wav\n",
      "source_2_prediction_sample_rate: 8000\n",
      "source_2_prediction_waveform.shape : torch.Size([1, 66080])\n",
      "source_2_prediction_waveform: tensor([1.7083e-04, 6.2734e-04, 1.2099e-03,  ..., 1.8495e-04, 1.4669e-04,\n",
      "        9.6560e-05])\n",
      "\n",
      "source_3_path : /storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s3/resampled_khz160004077-13754-0001_5142-33396-0065_5683-32866-0012.wav\n",
      "source_3_sample_rate:16000\n",
      "source_3_waveform.shape:torch.Size([1, 66080])\n",
      "source_3_waveform:tensor([-0.0123, -0.0183, -0.0161,  ...,  0.0041,  0.0038,  0.0121])\n",
      "\n",
      "source_3_path_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source3_resampled_khz160004077-13754-0001_5142-33396-0065_5683-32866-0012.wav\n",
      "source_3_prediction_sample_rate: 8000\n",
      "source_3_prediction_waveform.shape : torch.Size([1, 66080])\n",
      "source_3_prediction_waveform: tensor([-0.0080, -0.0129, -0.0148,  ...,  0.0007,  0.0003,  0.0025])\n",
      "preds.shape torch.Size([3, 66080])\n",
      "target.shape:torch.Size([3, 66080])\n",
      "preds: tensor([[ 2.3164e-02,  2.7920e-02,  2.3897e-02,  ..., -1.2341e-03,\n",
      "         -1.0093e-03, -8.3894e-04],\n",
      "        [ 1.7083e-04,  6.2734e-04,  1.2099e-03,  ...,  1.8495e-04,\n",
      "          1.4669e-04,  9.6560e-05],\n",
      "        [-7.9641e-03, -1.2885e-02, -1.4809e-02,  ...,  7.2891e-04,\n",
      "          3.3838e-04,  2.5319e-03]]),\n",
      "target:tensor([[-0.0114, -0.0171, -0.0155,  ...,  0.0044,  0.0043,  0.0128],\n",
      "        [-0.0107, -0.0163, -0.0144,  ...,  0.0044,  0.0043,  0.0128],\n",
      "        [-0.0123, -0.0183, -0.0161,  ...,  0.0041,  0.0038,  0.0121]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valeriopuglisi/.conda/envs/DLAABE/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (ScaleInvariantSignalNoiseRatio). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/valeriopuglisi/.conda/envs/DLAABE/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (ScaleInvariantSignalDistortionRatio). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Benchmarking: 2/3   tot 3000\n",
      "model_sample_rate:8000 - dataset_sample_rate:16000\n",
      "revisited mixture_path:/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/mix_both/resampled_khz160004507-16021-0025_1188-133604-0025_4992-23283-0016.wav\n",
      "revisited source_1_path:/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s1/resampled_khz160004507-16021-0025_1188-133604-0025_4992-23283-0016.wav\n",
      "revisited source_2_path:/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s2/resampled_khz160004507-16021-0025_1188-133604-0025_4992-23283-0016.wav\n",
      "revisited source_3_path:/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s3/resampled_khz160004507-16021-0025_1188-133604-0025_4992-23283-0016.wav\n",
      "source_1_path : /storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s1/resampled_khz160004507-16021-0025_1188-133604-0025_4992-23283-0016.wav\n",
      "source_1_sample_rate:16000\n",
      "source_1_waveform.shape:torch.Size([1, 73720])\n",
      "source_1_waveform:tensor([ 0.0056,  0.0217,  0.0132,  ..., -0.0058, -0.0213, -0.0251])\n",
      "\n",
      "source_1_path_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source1_resampled_khz160004507-16021-0025_1188-133604-0025_4992-23283-0016.wav\n",
      "source_1_prediction_sample_rate: 8000\n",
      "source_1_prediction_waveform.shape : torch.Size([1, 73720])\n",
      "source_1_prediction_waveform: tensor([ 0.0002, -0.0007, -0.0002,  ..., -0.0017,  0.0036,  0.0024])\n",
      "\n",
      "source_2_path : /storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s2/resampled_khz160004507-16021-0025_1188-133604-0025_4992-23283-0016.wav\n",
      "source_2_sample_rate:16000\n",
      "source_2_waveform.shape:torch.Size([1, 73720])\n",
      "source_2_waveform:tensor([ 0.0047,  0.0208,  0.0124,  ..., -0.0042, -0.0196, -0.0225])\n",
      "\n",
      "source_2_path_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source2_resampled_khz160004507-16021-0025_1188-133604-0025_4992-23283-0016.wav\n",
      "source_2_prediction_sample_rate: 8000\n",
      "source_2_prediction_waveform.shape : torch.Size([1, 73720])\n",
      "source_2_prediction_waveform: tensor([-0.0007, -0.0002,  0.0004,  ...,  0.0014, -0.0034, -0.0052])\n",
      "\n",
      "source_3_path : /storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s3/resampled_khz160004507-16021-0025_1188-133604-0025_4992-23283-0016.wav\n",
      "source_3_sample_rate:16000\n",
      "source_3_waveform.shape:torch.Size([1, 73720])\n",
      "source_3_waveform:tensor([ 0.0055,  0.0221,  0.0138,  ..., -0.0042, -0.0196, -0.0225])\n",
      "\n",
      "source_3_path_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source3_resampled_khz160004507-16021-0025_1188-133604-0025_4992-23283-0016.wav\n",
      "source_3_prediction_sample_rate: 8000\n",
      "source_3_prediction_waveform.shape : torch.Size([1, 73720])\n",
      "source_3_prediction_waveform: tensor([ 0.0117,  0.0225,  0.0176,  ..., -0.0117, -0.0414, -0.0385])\n",
      "preds.shape torch.Size([3, 73720])\n",
      "target.shape:torch.Size([3, 73720])\n",
      "preds: tensor([[ 0.0002, -0.0007, -0.0002,  ..., -0.0017,  0.0036,  0.0024],\n",
      "        [-0.0007, -0.0002,  0.0004,  ...,  0.0014, -0.0034, -0.0052],\n",
      "        [ 0.0117,  0.0225,  0.0176,  ..., -0.0117, -0.0414, -0.0385]]),\n",
      "target:tensor([[ 0.0056,  0.0217,  0.0132,  ..., -0.0058, -0.0213, -0.0251],\n",
      "        [ 0.0047,  0.0208,  0.0124,  ..., -0.0042, -0.0196, -0.0225],\n",
      "        [ 0.0055,  0.0221,  0.0138,  ..., -0.0042, -0.0196, -0.0225]])\n",
      "====> Benchmarking: 3/3   tot 3000\n",
      "model_sample_rate:8000 - dataset_sample_rate:16000\n",
      "revisited mixture_path:/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/mix_both/resampled_khz160005142-36377-0017_2830-3980-0022_3729-6852-0038.wav\n",
      "revisited source_1_path:/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s1/resampled_khz160005142-36377-0017_2830-3980-0022_3729-6852-0038.wav\n",
      "revisited source_2_path:/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s2/resampled_khz160005142-36377-0017_2830-3980-0022_3729-6852-0038.wav\n",
      "revisited source_3_path:/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s3/resampled_khz160005142-36377-0017_2830-3980-0022_3729-6852-0038.wav\n",
      "source_1_path : /storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s1/resampled_khz160005142-36377-0017_2830-3980-0022_3729-6852-0038.wav\n",
      "source_1_sample_rate:16000\n",
      "source_1_waveform.shape:torch.Size([1, 46160])\n",
      "source_1_waveform:tensor([ 0.0158,  0.0392,  0.0276,  ..., -0.0042,  0.0057,  0.0139])\n",
      "\n",
      "source_1_path_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source1_resampled_khz160005142-36377-0017_2830-3980-0022_3729-6852-0038.wav\n",
      "source_1_prediction_sample_rate: 8000\n",
      "source_1_prediction_waveform.shape : torch.Size([1, 46160])\n",
      "source_1_prediction_waveform: tensor([-0.0428, -0.0597, -0.0509,  ...,  0.0079, -0.0042, -0.0094])\n",
      "\n",
      "source_2_path : /storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s2/resampled_khz160005142-36377-0017_2830-3980-0022_3729-6852-0038.wav\n",
      "source_2_sample_rate:16000\n",
      "source_2_waveform.shape:torch.Size([1, 46160])\n",
      "source_2_waveform:tensor([ 0.0188,  0.0433,  0.0314,  ..., -0.0042,  0.0057,  0.0139])\n",
      "\n",
      "source_2_path_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source2_resampled_khz160005142-36377-0017_2830-3980-0022_3729-6852-0038.wav\n",
      "source_2_prediction_sample_rate: 8000\n",
      "source_2_prediction_waveform.shape : torch.Size([1, 46160])\n",
      "source_2_prediction_waveform: tensor([-0.0047, -0.0034, -0.0009,  ..., -0.0022, -0.0018, -0.0020])\n",
      "\n",
      "source_3_path : /storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s3/resampled_khz160005142-36377-0017_2830-3980-0022_3729-6852-0038.wav\n",
      "source_3_sample_rate:16000\n",
      "source_3_waveform.shape:torch.Size([1, 46160])\n",
      "source_3_waveform:tensor([ 0.0154,  0.0385,  0.0272,  ..., -0.0052,  0.0049,  0.0132])\n",
      "\n",
      "source_3_path_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source3_resampled_khz160005142-36377-0017_2830-3980-0022_3729-6852-0038.wav\n",
      "source_3_prediction_sample_rate: 8000\n",
      "source_3_prediction_waveform.shape : torch.Size([1, 46160])\n",
      "source_3_prediction_waveform: tensor([0.0023, 0.0141, 0.0170,  ..., 0.0058, 0.0121, 0.0130])\n",
      "preds.shape torch.Size([3, 46160])\n",
      "target.shape:torch.Size([3, 46160])\n",
      "preds: tensor([[-0.0428, -0.0597, -0.0509,  ...,  0.0079, -0.0042, -0.0094],\n",
      "        [-0.0047, -0.0034, -0.0009,  ..., -0.0022, -0.0018, -0.0020],\n",
      "        [ 0.0023,  0.0141,  0.0170,  ...,  0.0058,  0.0121,  0.0130]]),\n",
      "target:tensor([[ 0.0158,  0.0392,  0.0276,  ..., -0.0042,  0.0057,  0.0139],\n",
      "        [ 0.0188,  0.0433,  0.0314,  ..., -0.0042,  0.0057,  0.0139],\n",
      "        [ 0.0154,  0.0385,  0.0272,  ..., -0.0052,  0.0049,  0.0132]])\n",
      "============================================================================================\n",
      "total_si_snr:-6.253245830535889\n",
      "total_si_sdr:-6.282597064971924\n",
      "total_snr:-7.286540508270264\n",
      "total_sdr:-2.7099030017852783\n",
      "total_pit:0.0\n",
      "total_wb_pesq:0.0\n",
      "total_nb_pesq:1.759475827217102\n",
      "total_stoi:0.45118749141693115\n",
      "2022-09-29 13:45:15.452748\n",
      "result_filename :  29_Sep_2022__13_45_15_452748_evaluate_speech_separation_sepformer_wsj03mix_Libri3Mix16kMax_test_mix_both_file_3.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'model': '/api/audioseparation/speech_separation_sepformer_wsj03mix',\n",
       "  'dataset': 'Libri3Mix16kMax',\n",
       "  'n_test': 3,\n",
       "  'metrics_error': 0,\n",
       "  'model_sample_rate': 8000,\n",
       "  'dataset_sample_rate': 16000,\n",
       "  'n_test_done': 3,\n",
       "  'total_si_snr': 'tensor(-6.2532)',\n",
       "  'total_si_sdr': 'tensor(-6.2826)',\n",
       "  'total_snr': 'tensor(-7.2865)',\n",
       "  'total_sdr': 'tensor(-2.7099)',\n",
       "  'total_pit': 'tensor(0.)',\n",
       "  'total_wb_pesq': 'tensor(0.)',\n",
       "  'total_nb_pesq': 'tensor(1.7595)',\n",
       "  'total_stoi': 'tensor(0.4512)',\n",
       "  'experiments': {'0': {'mixture_path': '/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/mix_both/4077-13754-0001_5142-33396-0065_5683-32866-0012.wav',\n",
       "    'source_1_path': '/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s1/4077-13754-0001_5142-33396-0065_5683-32866-0012.wav',\n",
       "    'source_2_path': '/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s2/4077-13754-0001_5142-33396-0065_5683-32866-0012.wav',\n",
       "    'source_3_path': '/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s3/4077-13754-0001_5142-33396-0065_5683-32866-0012.wav',\n",
       "    'source_1_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source1_resampled_khz160004077-13754-0001_5142-33396-0065_5683-32866-0012.wav',\n",
       "    'source_2_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source2_resampled_khz160004077-13754-0001_5142-33396-0065_5683-32866-0012.wav',\n",
       "    'source_3_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source3_resampled_khz160004077-13754-0001_5142-33396-0065_5683-32866-0012.wav',\n",
       "    'si-snr': 'tensor([[6.0980]])',\n",
       "    'si-sdr': 'tensor([[6.0963]])',\n",
       "    'sdr': 'tensor([[7.6196]])',\n",
       "    'snr': 'tensor([[-5.0469]])',\n",
       "    'pesq': 'tensor([[2.2620]])',\n",
       "    'stoi': 'tensor([[0.6395]])'},\n",
       "   '1': {'mixture_path': '/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/mix_both/4507-16021-0025_1188-133604-0025_4992-23283-0016.wav',\n",
       "    'source_1_path': '/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s1/4507-16021-0025_1188-133604-0025_4992-23283-0016.wav',\n",
       "    'source_2_path': '/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s2/4507-16021-0025_1188-133604-0025_4992-23283-0016.wav',\n",
       "    'source_3_path': '/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s3/4507-16021-0025_1188-133604-0025_4992-23283-0016.wav',\n",
       "    'source_1_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source1_resampled_khz160004507-16021-0025_1188-133604-0025_4992-23283-0016.wav',\n",
       "    'source_2_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source2_resampled_khz160004507-16021-0025_1188-133604-0025_4992-23283-0016.wav',\n",
       "    'source_3_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source3_resampled_khz160004507-16021-0025_1188-133604-0025_4992-23283-0016.wav',\n",
       "    'si-snr': 'tensor([[-18.0557]])',\n",
       "    'si-sdr': 'tensor([[-18.0566]])',\n",
       "    'sdr': 'tensor([[-10.0900]])',\n",
       "    'snr': 'tensor([[-9.3970]])',\n",
       "    'pesq': 'tensor([[1.4470]])',\n",
       "    'stoi': 'tensor([[0.3347]])'},\n",
       "   '2': {'mixture_path': '/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/mix_both/5142-36377-0017_2830-3980-0022_3729-6852-0038.wav',\n",
       "    'source_1_path': '/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s1/5142-36377-0017_2830-3980-0022_3729-6852-0038.wav',\n",
       "    'source_2_path': '/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s2/5142-36377-0017_2830-3980-0022_3729-6852-0038.wav',\n",
       "    'source_3_path': '/storage/data_8T/datasets/audio/LibriMix/Libri3Mix/wav16k/max/test/s3/5142-36377-0017_2830-3980-0022_3729-6852-0038.wav',\n",
       "    'source_1_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source1_resampled_khz160005142-36377-0017_2830-3980-0022_3729-6852-0038.wav',\n",
       "    'source_2_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source2_resampled_khz160005142-36377-0017_2830-3980-0022_3729-6852-0038.wav',\n",
       "    'source_3_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj03mix/SpeechSeparationSepformerWsj03mix_source3_resampled_khz160005142-36377-0017_2830-3980-0022_3729-6852-0038.wav',\n",
       "    'si-snr': 'tensor([[-6.8020]])',\n",
       "    'si-sdr': 'tensor([[-6.8875]])',\n",
       "    'sdr': 'tensor([[-5.6593]])',\n",
       "    'snr': 'tensor([[-7.4157]])',\n",
       "    'pesq': 'tensor([[1.5694]])',\n",
       "    'stoi': 'tensor([[0.3793]])'}}},\n",
       " 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def speech_separation_evaluate_metric_with_model_on_libri3mix(model, dataset, metrics, n_test, mix_type=\"test_mix_clean_file\"):\n",
    "    task = \"Speech Separation\"\n",
    "    total_si_snr = torch.zeros(0)\n",
    "    total_si_sdr = torch.zeros(0)\n",
    "    total_snr = torch.zeros(0)\n",
    "    total_sdr = torch.zeros(0)\n",
    "    total_nb_pesq = torch.zeros(0)\n",
    "    total_wb_pesq = torch.zeros(0)\n",
    "    total_pit = torch.zeros(0)\n",
    "    total_stoi= torch.zeros(0)\n",
    "    metrics_error = 0 \n",
    "    experiments = {\n",
    "    }\n",
    "\n",
    "    result = {}\n",
    "    # os.walk(dataset_path)\n",
    "    print(\"===================================================>  Dataset: {}\".format(dataset))\n",
    "    # print(Datasets[task])\n",
    "    print(\"Datasets[task][dataset] : {}\".format(Datasets[task][dataset][mix_type]))\n",
    "    test_table = pd.read_table(Datasets[task][dataset][mix_type], sep=\",\")\n",
    "    # display(test_table)\n",
    "    # cols = test_table.iloc[:,1:4]\n",
    "    print(\"len(test_table): {}\".format(len(test_table)))\n",
    "    # display(cols)\n",
    "    for i, row in enumerate(test_table.iterrows()):\n",
    "        preds = torch.zeros(0) \n",
    "        target = torch.zeros(0)\n",
    "        print(\"====> Benchmarking: {}/{}   tot {}\".format(i+1, n_test, test_table.shape[0]))\n",
    "        # print(i, row[1]['mixture_path'], , , row[1]['noise_path'])\n",
    "\n",
    "        model_channels = AudioAnalysisAPI[model]['channels']\n",
    "        dataset_channels = Datasets[task][dataset]['channels']\n",
    "        if model_channels != dataset_channels:\n",
    "            return {\"error\": \"Model and Dataset have differents channels number\"}\n",
    "\n",
    "\n",
    "        mixture_path = row[1]['mixture_path']\n",
    "        source_1_path = row[1]['source_1_path']\n",
    "        source_2_path = row[1]['source_2_path']\n",
    "        source_3_path = row[1]['source_3_path']\n",
    "\n",
    "        #Taking model sample rate\n",
    "        model_sample_rate = AudioAnalysisAPI[model]['sample_rate']\n",
    "        dataset_sample_rate = Datasets[task][dataset]['sample_rate']\n",
    "        print(\"model_sample_rate:{} - dataset_sample_rate:{}\".format(model_sample_rate, dataset_sample_rate))\n",
    "        # Writing on experiment original paths\n",
    "        experiments[str(i)]= {\n",
    "            \"mixture_path\":mixture_path,\n",
    "            \"source_1_path\":source_1_path,\n",
    "            \"source_2_path\":source_2_path,\n",
    "            \"source_3_path\":source_3_path,\n",
    "        }\n",
    "        # Loading targets audio on tensors\n",
    "        mixture_waveform, mixture_sample_rate = torchaudio.load(mixture_path)\n",
    "        source_1_waveform, source_1_sample_rate = torchaudio.load(source_1_path)\n",
    "        source_2_waveform, source_2_sample_rate = torchaudio.load(source_2_path)\n",
    "        source_3_waveform, source_3_sample_rate = torchaudio.load(source_3_path)\n",
    "        \n",
    "\n",
    "        if model_sample_rate != dataset_sample_rate:\n",
    "            mixture_waveform = TAF.resample(mixture_waveform, mixture_sample_rate, model_sample_rate)\n",
    "            source_1_waveform = TAF.resample(source_1_waveform, source_1_sample_rate, model_sample_rate)\n",
    "            source_2_waveform = TAF.resample(source_2_waveform, source_2_sample_rate, model_sample_rate)\n",
    "            source_3_waveform = TAF.resample(source_3_waveform, source_3_sample_rate, model_sample_rate)\n",
    "            mixture_path = os.path.join(os.path.split(mixture_path)[0] ,   \"resampled_khz\" + str(dataset_sample_rate) +   os.path.split(mixture_path)[1])\n",
    "            source_1_path = os.path.join(os.path.split(source_1_path)[0] , \"resampled_khz\" + str(dataset_sample_rate) + os.path.split(source_1_path)[1])\n",
    "            source_2_path = os.path.join(os.path.split(source_2_path)[0] , \"resampled_khz\" + str(dataset_sample_rate) + os.path.split(source_2_path)[1])\n",
    "            source_3_path = os.path.join(os.path.split(source_3_path)[0] , \"resampled_khz\" + str(dataset_sample_rate) + os.path.split(source_3_path)[1])\n",
    "            print(\"revisited mixture_path:{}\".format(mixture_path))\n",
    "            print(\"revisited source_1_path:{}\".format(source_1_path))\n",
    "            print(\"revisited source_2_path:{}\".format(source_2_path))\n",
    "            print(\"revisited source_3_path:{}\".format(source_3_path))\n",
    "            torchaudio.save(mixture_path, mixture_waveform, model_sample_rate)\n",
    "            torchaudio.save(source_1_path, source_1_waveform, model_sample_rate)\n",
    "            torchaudio.save(source_2_path, source_2_waveform, model_sample_rate)\n",
    "            torchaudio.save(source_3_path, source_3_waveform, model_sample_rate)\n",
    "\n",
    "            if mix_type == \"test_mix_both_file\":\n",
    "                noise_path = row[1]['noise_path']\n",
    "                noise_waveform, noise_sample_rate = torchaudio.load(noise_path)\n",
    "                noise_waveform = TAF.resample(noise_waveform, noise_sample_rate, model_sample_rate)\n",
    "                source_1_waveform = source_1_waveform + noise_waveform\n",
    "                source_2_waveform = source_2_waveform + noise_waveform\n",
    "                source_3_waveform = source_3_waveform + noise_waveform\n",
    "        else:\n",
    "            if mix_type == \"test_mix_both_file\":\n",
    "                noise_path = row[1]['noise_path']\n",
    "                noise_waveform, noise_sample_rate = torchaudio.load(noise_path)\n",
    "                source_1_waveform = source_1_waveform + noise_waveform\n",
    "                source_2_waveform = source_2_waveform + noise_waveform\n",
    "                source_3_waveform = source_3_waveform + noise_waveform\n",
    "\n",
    "        # Separate audio files with choosen model\n",
    "        source_1_path_prediction, source_2_path_prediction, source_3_path_prediction = AudioAnalysisAPI[model]['function'](audiofile_path=mixture_path)\n",
    "        \n",
    "        # Loading predictions audio on tensors\n",
    "        source_1_prediction_waveform, source_1_prediction_sample_rate = torchaudio.load(source_1_path_prediction)\n",
    "        source_2_prediction_waveform, source_2_prediction_sample_rate = torchaudio.load(source_2_path_prediction)\n",
    "        source_3_prediction_waveform, source_3_prediction_sample_rate = torchaudio.load(source_3_path_prediction)\n",
    "        \n",
    "            \n",
    "        # Concatenating predictions into torch tensor \n",
    "        preds = torch.cat((preds, source_1_prediction_waveform), 0)\n",
    "        preds = torch.cat((preds, source_2_prediction_waveform), 0)\n",
    "        preds = torch.cat((preds, source_3_prediction_waveform), 0)\n",
    "        # Concatenating targets into torch tensor \n",
    "        target = torch.cat((target, source_1_waveform), 0)\n",
    "        target = torch.cat((target, source_2_waveform), 0)\n",
    "        target = torch.cat((target, source_3_waveform), 0)\n",
    "\n",
    "        \n",
    "       \n",
    "        # print(\"mixture_path : {}\".format(mixture_path))\n",
    "        # print(\"mixture_sample_rate:{}\".format(mixture_sample_rate))\n",
    "        # print(\"mixture_waveform.shape:{}\".format(mixture_waveform.shape))\n",
    "        # print(\"mixture_waveform:{}\".format(mixture_waveform))\n",
    "        # print()\n",
    "        print(\"source_1_path : {}\".format(source_1_path))\n",
    "        print(\"source_1_sample_rate:{}\".format(source_1_sample_rate))\n",
    "        print(\"source_1_waveform.shape:{}\".format(source_1_waveform.shape))\n",
    "        print(\"source_1_waveform:{}\".format(target[0]))\n",
    "        print()\n",
    "        print(\"source_1_path_prediction : {}\".format(source_1_path_prediction))\n",
    "        print(\"source_1_prediction_sample_rate: {}\".format(source_1_prediction_sample_rate))\n",
    "        print(\"source_1_prediction_waveform.shape : {}\".format(source_1_prediction_waveform.shape))\n",
    "        print(\"source_1_prediction_waveform: {}\".format(preds[0]))\n",
    "        print()\n",
    "        print(\"source_2_path : {}\".format(source_2_path))\n",
    "        print(\"source_2_sample_rate:{}\".format(source_2_sample_rate))\n",
    "        print(\"source_2_waveform.shape:{}\".format(source_2_waveform.shape))\n",
    "        print(\"source_2_waveform:{}\".format(target[1]))\n",
    "        print()\n",
    "        print(\"source_2_path_prediction : {}\".format(source_2_path_prediction))\n",
    "        print(\"source_2_prediction_sample_rate: {}\".format(source_2_prediction_sample_rate))\n",
    "        print(\"source_2_prediction_waveform.shape : {}\".format(source_2_prediction_waveform.shape))\n",
    "        print(\"source_2_prediction_waveform: {}\".format(preds[1]))\n",
    "        print()\n",
    "        print(\"source_3_path : {}\".format(source_3_path))\n",
    "        print(\"source_3_sample_rate:{}\".format(source_3_sample_rate))\n",
    "        print(\"source_3_waveform.shape:{}\".format(source_3_waveform.shape))\n",
    "        print(\"source_3_waveform:{}\".format(target[2]))\n",
    "        print()\n",
    "        print(\"source_3_path_prediction : {}\".format(source_3_path_prediction))\n",
    "        print(\"source_3_prediction_sample_rate: {}\".format(source_3_prediction_sample_rate))\n",
    "        print(\"source_3_prediction_waveform.shape : {}\".format(source_3_prediction_waveform.shape))\n",
    "        print(\"source_3_prediction_waveform: {}\".format(preds[2]))\n",
    "\n",
    "        print(\"preds.shape {}\\ntarget.shape:{}\".format(preds.shape, target.shape))\n",
    "        print(\"preds: {},\\ntarget:{}\".format(preds, target))\n",
    "        \n",
    "        experiments[str(i)][\"source_1_path_prediction\"] = source_1_path_prediction\n",
    "        experiments[str(i)][\"source_2_path_prediction\"] = source_2_path_prediction\n",
    "        experiments[str(i)][\"source_3_path_prediction\"] = source_3_path_prediction\n",
    "\n",
    "        \n",
    "        try:\n",
    "            for metric in metrics:\n",
    "            #print(\"Calculating metric:\", metric)\n",
    "                if metric == \"si-snr\":\n",
    "                    si_snr = ScaleInvariantSignalNoiseRatio()\n",
    "                    si_snr_result = si_snr(preds, target)\n",
    "                    si_snr_result = torch.reshape(si_snr_result, (1, 1))\n",
    "                    total_si_snr = torch.cat((total_si_snr, si_snr_result))\n",
    "                    experiments[str(i)][\"si-snr\"] = str(si_snr_result)\n",
    "\n",
    "                if metric == \"si-sdr\":\n",
    "                    si_sdr = ScaleInvariantSignalDistortionRatio()\n",
    "                    si_sdr_result = si_sdr(preds, target)\n",
    "                    si_sdr_result = torch.reshape(si_sdr_result, (1, 1))\n",
    "                    total_si_sdr = torch.cat((total_si_sdr, si_sdr_result)) \n",
    "                    experiments[str(i)][\"si-sdr\"] = str(si_sdr_result)                \n",
    "\n",
    "                if metric == \"snr\":\n",
    "                    snr = SignalNoiseRatio()\n",
    "                    snr_result = snr(preds, target)\n",
    "                    snr_result = torch.reshape(snr_result, (1, 1))\n",
    "                    total_snr = torch.cat((total_snr, snr_result))\n",
    "                    experiments[str(i)][\"snr\"] = str(snr_result)\n",
    "                    \n",
    "                if metric == \"sdr\":\n",
    "                    sdr = SignalDistortionRatio()\n",
    "                    sdr_result = sdr(preds, target)\n",
    "                    sdr_result = torch.reshape(sdr_result, (1, 1))\n",
    "                    total_sdr = torch.cat((total_sdr, sdr_result))\n",
    "                    experiments[str(i)][\"sdr\"] = str(sdr_result)\n",
    "\n",
    "                if metric == \"pesq\":\n",
    "                    nb_pesq = PerceptualEvaluationSpeechQuality(model_sample_rate, 'nb')\n",
    "                    nb_pesq_result = nb_pesq(preds, target)\n",
    "                    nb_pesq_result = torch.reshape(nb_pesq_result, (1, 1))\n",
    "                    total_nb_pesq = torch.cat((total_nb_pesq, nb_pesq_result))\n",
    "                    experiments[str(i)][\"pesq\"] = str(nb_pesq_result)\n",
    "\n",
    "                    if model_sample_rate > 8000: \n",
    "                        wb_pesq = PerceptualEvaluationSpeechQuality(model_sample_rate, 'wb')\n",
    "                        wb_pesq_result = wb_pesq(preds, target)\n",
    "                        wb_pesq_result = torch.reshape(wb_pesq_result, (1, 1))\n",
    "                        total_wb_pesq = torch.cat((total_wb_pesq, wb_pesq_result))\n",
    "                        experiments[str(i)][\"wb-pesq\"] = str(wb_pesq_result)\n",
    "\n",
    "                if metric == \"pit\":\n",
    "                    pit = PermutationInvariantTraining(signal_distortion_ratio, 'max')\n",
    "                    pit_result = pit(preds, target)\n",
    "                    pit_result = torch.reshape(pit_result, (1, 1))\n",
    "                    total_pit = torch.cat((total_pit, pit_result))\n",
    "                    experiments[str(i)][\"pit\"] = str(pit_result)\n",
    "\n",
    "                if metric == \"stoi\":\n",
    "                    stoi_src = ShortTimeObjectiveIntelligibility(model_sample_rate, False)\n",
    "                    stoi_result = stoi_src(preds, target)\n",
    "                    stoi_result = torch.reshape(stoi_result, (1, 1))\n",
    "                    total_stoi = torch.cat((total_stoi, stoi_result))\n",
    "                    experiments[str(i)][\"stoi\"] = str(stoi_result)\n",
    "        except Exception as e:\n",
    "            print(\"=====> ERROR: {}\".format(e))\n",
    "            metrics_error += 1\n",
    "\n",
    "\n",
    "\n",
    "        if i == n_test - 1: \n",
    "            break\n",
    "\n",
    "    \n",
    "    total_si_snr = torch.sum(total_si_snr)/n_test\n",
    "    total_si_sdr = torch.sum(total_si_sdr)/n_test\n",
    "    total_snr = torch.sum(total_snr)/n_test\n",
    "    total_sdr = torch.sum(total_sdr)/n_test\n",
    "    total_pit = torch.sum(total_pit)/n_test\n",
    "    total_wb_pesq = torch.sum(total_wb_pesq)/n_test\n",
    "    total_nb_pesq = torch.sum(total_nb_pesq)/n_test\n",
    "    total_stoi = torch.sum(total_stoi)/n_test\n",
    "\n",
    "    print(\"============================================================================================\")\n",
    "    print(\"total_si_snr:{}\".format(total_si_snr)) \n",
    "    print(\"total_si_sdr:{}\".format(total_si_sdr)) \n",
    "    print(\"total_snr:{}\".format(total_snr)) \n",
    "    print(\"total_sdr:{}\".format(total_sdr)) \n",
    "    print(\"total_pit:{}\".format(total_pit)) \n",
    "    print(\"total_wb_pesq:{}\".format(total_wb_pesq))  \n",
    "    print(\"total_nb_pesq:{}\".format(total_nb_pesq))  \n",
    "    print(\"total_stoi:{}\".format(total_stoi))  \n",
    "    # Creating the content of result file\n",
    "    result ={\n",
    "        \"model\": model, \n",
    "        \"dataset\": dataset, \n",
    "        \"n_test\": n_test,\n",
    "        \"metrics_error\": metrics_error,\n",
    "        \"model_sample_rate\":model_sample_rate,\n",
    "        \"dataset_sample_rate\":dataset_sample_rate,\n",
    "        \"n_test_done\": n_test - metrics_error,\n",
    "        \"total_si_snr\":str(total_si_snr),\n",
    "        \"total_si_sdr\":str(total_si_sdr),\n",
    "        \"total_snr\":str(total_snr),\n",
    "        \"total_sdr\":str(total_sdr),\n",
    "        \"total_pit\":str(total_pit),\n",
    "        \"total_wb_pesq\":str(total_wb_pesq),\n",
    "        \"total_nb_pesq\":str(total_nb_pesq),\n",
    "        \"total_stoi\":str(total_stoi),\n",
    "        \"experiments\": experiments,\n",
    "    }\n",
    "\n",
    "    # Creating filename\n",
    "    dateTimeObj = datetime.now()\n",
    "    print(dateTimeObj)\n",
    "    timestampStr = dateTimeObj.strftime(\"%d_%b_%Y__%H_%M_%S_%f\")\n",
    "    result_filename = timestampStr + \"_evaluate_\" + model.split(\"/\")[-1] + \"_\" + dataset + \"_\" + mix_type + \"_\" + str(n_test)+\".json\"\n",
    "    print('result_filename : ', result_filename)\n",
    "    with open(result_filename, 'w') as f:\n",
    "        json.dump(result, f)\n",
    "        \n",
    "    return result, n_test\n",
    "\n",
    "speech_separation_3_channels_models = ['/api/audioseparation/speech_separation_sepformer_wsj03mix']\n",
    "speech_separation_dataset = [\"Libri3Mix8kMin\", \"Libri3Mix8kMax\", \"Libri3Mix16kMin\", \"Libri3Mix16kMax\"]\n",
    "metrics = [\"si-snr\", \"si-sdr\", \"sdr\", \"snr\", \"pesq\", \"stoi\"]\n",
    "n_test = 3\n",
    "\n",
    "# type = \"test_mix_clean_file\" #\"test_mix_both_file\" \"test_mix_single_file\"\n",
    "type = \"test_mix_both_file\" \n",
    "# type = \"test_mix_single_file\"\n",
    "\n",
    "# speech_separation_evaluate_metric_with_model_on_librimix_2_channels(\n",
    "#     speech_separation_2_channels_models[0], \n",
    "#     speech_separation_dataset[0],\n",
    "#     [metrics[0], metrics[1], metrics[2], metrics[3], metrics[4], metrics[6]], \n",
    "#     n_test=n_test )\n",
    "speech_separation_evaluate_metric_with_model_on_libri3mix(speech_separation_3_channels_models[0], speech_separation_dataset[0], metrics, n_test=n_test, mix_type=type )\n",
    "speech_separation_evaluate_metric_with_model_on_libri3mix(speech_separation_3_channels_models[0], speech_separation_dataset[1], metrics, n_test=n_test, mix_type=type )\n",
    "speech_separation_evaluate_metric_with_model_on_libri3mix(speech_separation_3_channels_models[0], speech_separation_dataset[2], metrics, n_test=n_test, mix_type=type )\n",
    "speech_separation_evaluate_metric_with_model_on_libri3mix(speech_separation_3_channels_models[0], speech_separation_dataset[3], metrics, n_test=n_test, mix_type=type )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of '/home/User/Desktop/file.txt:' /home/User/Desktop\n",
      "Tail of '/home/User/Desktop/file.txt:' file.txt \n",
      "\n",
      "Head of '/home/User/Desktop/:' /home/User/Desktop\n",
      "Tail of '/home/User/Desktop/:'  \n",
      "\n",
      "Head of 'file.txt:' \n",
      "Tail of 'file.txt:' file.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Python program to explain os.path.split() method \n",
    "    \n",
    "# importing os module \n",
    "import os\n",
    "  \n",
    "# path\n",
    "path = '/home/User/Desktop/file.txt'\n",
    "  \n",
    "# Split the path in \n",
    "# head and tail pair\n",
    "head_tail = os.path.split(path)\n",
    "  \n",
    "# print head and tail\n",
    "# of the specified path\n",
    "print(\"Head of '% s:'\" % path, head_tail[0])\n",
    "print(\"Tail of '% s:'\" % path, head_tail[1], \"\\n\")\n",
    "  \n",
    "  \n",
    "# path\n",
    "path = '/home/User/Desktop/'\n",
    "  \n",
    "# Split the path in \n",
    "# head and tail pair\n",
    "head_tail = os.path.split(path)\n",
    "  \n",
    "# print head and tail\n",
    "# of the specified path\n",
    "print(\"Head of '% s:'\" % path, head_tail[0])\n",
    "print(\"Tail of '% s:'\" % path, head_tail[1], \"\\n\")\n",
    "  \n",
    "# path\n",
    "path = 'file.txt'\n",
    "  \n",
    "# Split the path in \n",
    "# head and tail pair\n",
    "head_tail = os.path.split(path)\n",
    "  \n",
    "# print head and tail\n",
    "# of the specified path\n",
    "print(\"Head of '% s:'\" % path, head_tail[0])\n",
    "print(\"Tail of '% s:'\" % path, head_tail[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DLAABE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b806cf2415d9a7125fea09a144cd06efbd6f0375c85c1aa682e3c6e04c90e28f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
