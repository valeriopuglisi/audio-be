{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valeriopuglisi/.conda/envs/DLAABE/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from deep_learning_features_audio import *\n",
    "from deep_learning_dict_api import AudioAnalysisAPI\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from torchmetrics import ScaleInvariantSignalNoiseRatio, ScaleInvariantSignalDistortionRatio, SignalNoiseRatio, SignalDistortionRatio, PermutationInvariantTraining\n",
    "from torchmetrics.audio.pesq import PerceptualEvaluationSpeechQuality\n",
    "from torchmetrics.functional.audio import signal_distortion_ratio\n",
    "from torchmetrics.audio.stoi import ShortTimeObjectiveIntelligibility\n",
    "from datetime import datetime\n",
    "from deep_learning_dict_datasets import Datasets\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cu102\n",
      "0.11.0+cu102\n"
     ]
    }
   ],
   "source": [
    "# When running this tutorial in Google Colab, install the required packages\n",
    "# with the following.\n",
    "# !pip install torchaudio librosa boto3\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as TAF\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================>  Dataset: Libri2Mix16kMax\n",
      "Datasets[task][dataset] : /storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/max/metadata/mixture_test_mix_both.csv\n",
      "len(test_table): 3000\n",
      "====> Benchmarking: 1/3   tot 3000\n",
      "model_sample_rate:8000 - dataset_sample_rate:16000\n",
      "mixture_path : evaluation_target_mixture.wav\n",
      "mixture_sample_rate:16000\n",
      "mixture_waveform.shape:torch.Size([1, 41560])\n",
      "mixture_waveform:tensor([[ 0.0079,  0.0128,  0.0075,  ..., -0.0018, -0.0005,  0.0052]])\n",
      "\n",
      "source_1_path : evaluation_target_source_1.wav\n",
      "source_1_sample_rate:16000\n",
      "source_1_waveform.shape:torch.Size([1, 41560])\n",
      "source_1_waveform:tensor([ 0.0069,  0.0122,  0.0068,  ..., -0.0018, -0.0005,  0.0055])\n",
      "\n",
      "source_1_path_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source1_evaluation_target_mixture.wav\n",
      "source_1_prediction_sample_rate: 8000\n",
      "source_1_prediction_waveform.shape : torch.Size([1, 47040])\n",
      "source_1_prediction_waveform: tensor([0.0125, 0.0261, 0.0304,  ..., 0.1207, 0.1043, 0.0860])\n",
      "\n",
      "source_2_path : evaluation_target_source_2.wav\n",
      "source_2_sample_rate:16000\n",
      "source_2_waveform.shape:torch.Size([1, 41560])\n",
      "source_2_waveform:tensor([ 0.0081,  0.0134,  0.0084,  ..., -0.0018, -0.0005,  0.0052])\n",
      "\n",
      "source_2_path_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source2_evaluation_target_mixture.wav\n",
      "source_2_prediction_sample_rate: 8000\n",
      "source_2_prediction_waveform.shape : torch.Size([1, 47040])\n",
      "source_2_prediction_waveform: tensor([-0.0003, -0.0002, -0.0013,  ...,  0.0026, -0.0040,  0.0032])\n",
      "preds.shape torch.Size([2, 47040])\n",
      "target.shape:torch.Size([2, 41560])\n",
      "preds: tensor([[ 0.0125,  0.0261,  0.0304,  ...,  0.1207,  0.1043,  0.0860],\n",
      "        [-0.0003, -0.0002, -0.0013,  ...,  0.0026, -0.0040,  0.0032]]),\n",
      "target:tensor([[ 0.0069,  0.0122,  0.0068,  ..., -0.0018, -0.0005,  0.0055],\n",
      "        [ 0.0081,  0.0134,  0.0084,  ..., -0.0018, -0.0005,  0.0052]])\n",
      "=====> ERROR: Predictions and targets are expected to have the same shape\n",
      "====> Benchmarking: 2/3   tot 3000\n",
      "model_sample_rate:8000 - dataset_sample_rate:16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valeriopuglisi/.conda/envs/DLAABE/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (ScaleInvariantSignalNoiseRatio). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixture_path : evaluation_target_mixture.wav\n",
      "mixture_sample_rate:16000\n",
      "mixture_waveform.shape:torch.Size([1, 73680])\n",
      "mixture_waveform:tensor([[-0.0232, -0.0228, -0.0292,  ...,  0.0399,  0.0422,  0.0337]])\n",
      "\n",
      "source_1_path : evaluation_target_source_1.wav\n",
      "source_1_sample_rate:16000\n",
      "source_1_waveform.shape:torch.Size([1, 73680])\n",
      "source_1_waveform:tensor([-0.0235, -0.0232, -0.0295,  ...,  0.0404,  0.0426,  0.0342])\n",
      "\n",
      "source_1_path_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source1_evaluation_target_mixture.wav\n",
      "source_1_prediction_sample_rate: 8000\n",
      "source_1_prediction_waveform.shape : torch.Size([1, 73680])\n",
      "source_1_prediction_waveform: tensor([ 0.0035,  0.0070,  0.0136,  ..., -0.0557, -0.0362, -0.0241])\n",
      "\n",
      "source_2_path : evaluation_target_source_2.wav\n",
      "source_2_sample_rate:16000\n",
      "source_2_waveform.shape:torch.Size([1, 73680])\n",
      "source_2_waveform:tensor([-0.0222, -0.0221, -0.0294,  ...,  0.0399,  0.0422,  0.0337])\n",
      "\n",
      "source_2_path_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source2_evaluation_target_mixture.wav\n",
      "source_2_prediction_sample_rate: 8000\n",
      "source_2_prediction_waveform.shape : torch.Size([1, 73680])\n",
      "source_2_prediction_waveform: tensor([-0.0070, -0.0119, -0.0213,  ...,  0.0663,  0.0680,  0.0392])\n",
      "preds.shape torch.Size([2, 73680])\n",
      "target.shape:torch.Size([2, 73680])\n",
      "preds: tensor([[ 0.0035,  0.0070,  0.0136,  ..., -0.0557, -0.0362, -0.0241],\n",
      "        [-0.0070, -0.0119, -0.0213,  ...,  0.0663,  0.0680,  0.0392]]),\n",
      "target:tensor([[-0.0235, -0.0232, -0.0295,  ...,  0.0404,  0.0426,  0.0342],\n",
      "        [-0.0222, -0.0221, -0.0294,  ...,  0.0399,  0.0422,  0.0337]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valeriopuglisi/.conda/envs/DLAABE/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (ScaleInvariantSignalDistortionRatio). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Benchmarking: 3/3   tot 3000\n",
      "model_sample_rate:8000 - dataset_sample_rate:16000\n",
      "mixture_path : evaluation_target_mixture.wav\n",
      "mixture_sample_rate:16000\n",
      "mixture_waveform.shape:torch.Size([1, 59600])\n",
      "mixture_waveform:tensor([[-0.0092, -0.0107, -0.0064,  ...,  0.0108,  0.0225,  0.0287]])\n",
      "\n",
      "source_1_path : evaluation_target_source_1.wav\n",
      "source_1_sample_rate:16000\n",
      "source_1_waveform.shape:torch.Size([1, 59600])\n",
      "source_1_waveform:tensor([-0.0093, -0.0108, -0.0065,  ...,  0.0108,  0.0225,  0.0286])\n",
      "\n",
      "source_1_path_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source1_evaluation_target_mixture.wav\n",
      "source_1_prediction_sample_rate: 8000\n",
      "source_1_prediction_waveform.shape : torch.Size([1, 59600])\n",
      "source_1_prediction_waveform: tensor([0.0029, 0.0033, 0.0008,  ..., 0.0013, 0.0002, 0.0009])\n",
      "\n",
      "source_2_path : evaluation_target_source_2.wav\n",
      "source_2_sample_rate:16000\n",
      "source_2_waveform.shape:torch.Size([1, 59600])\n",
      "source_2_waveform:tensor([-0.0079, -0.0087, -0.0042,  ...,  0.0131,  0.0250,  0.0309])\n",
      "\n",
      "source_2_path_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source2_evaluation_target_mixture.wav\n",
      "source_2_prediction_sample_rate: 8000\n",
      "source_2_prediction_waveform.shape : torch.Size([1, 59600])\n",
      "source_2_prediction_waveform: tensor([-0.0129, -0.0162, -0.0110,  ...,  0.0178,  0.0373,  0.0361])\n",
      "preds.shape torch.Size([2, 59600])\n",
      "target.shape:torch.Size([2, 59600])\n",
      "preds: tensor([[ 0.0029,  0.0033,  0.0008,  ...,  0.0013,  0.0002,  0.0009],\n",
      "        [-0.0129, -0.0162, -0.0110,  ...,  0.0178,  0.0373,  0.0361]]),\n",
      "target:tensor([[-0.0093, -0.0108, -0.0065,  ...,  0.0108,  0.0225,  0.0286],\n",
      "        [-0.0079, -0.0087, -0.0042,  ...,  0.0131,  0.0250,  0.0309]])\n",
      "============================================================================================\n",
      "total_si_snr:-6.809534072875977\n",
      "total_si_sdr:-6.821482181549072\n",
      "total_snr:-4.99863338470459\n",
      "total_sdr:-3.9104175567626953\n",
      "total_pit:0.0\n",
      "total_wb_pesq:0.0\n",
      "total_nb_pesq:1.1074212789535522\n",
      "total_stoi:0.2738586366176605\n",
      "2022-09-29 10:50:26.608954\n",
      "result_filename :  29_Sep_2022__10_50_26_608954_evaluate_speech_separation_sepformer_wsj02mix_Libri2Mix16kMax_test_mix_both_file_3.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'model': '/api/audioseparation/speech_separation_sepformer_wsj02mix',\n",
       "  'dataset': 'Libri2Mix16kMax',\n",
       "  'n_test': 3,\n",
       "  'metrics_error': 1,\n",
       "  'model_sample_rate': 8000,\n",
       "  'dataset_sample_rate': 8000,\n",
       "  'n_test_done': 2,\n",
       "  'total_si_snr': 'tensor(-6.8095)',\n",
       "  'total_si_sdr': 'tensor(-6.8215)',\n",
       "  'total_snr': 'tensor(-4.9986)',\n",
       "  'total_sdr': 'tensor(-3.9104)',\n",
       "  'total_pit': 'tensor(0.)',\n",
       "  'total_wb_pesq': 'tensor(0.)',\n",
       "  'total_nb_pesq': 'tensor(1.1074)',\n",
       "  'total_stoi': 'tensor(0.2739)',\n",
       "  'experiments': {'0': {'mixture_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/max/test/mix_both/4077-13754-0001_5142-33396-0065.wav',\n",
       "    'source_1_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/max/test/s1/4077-13754-0001_5142-33396-0065.wav',\n",
       "    'source_2_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/max/test/s2/4077-13754-0001_5142-33396-0065.wav',\n",
       "    'source_1_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source1_evaluation_target_mixture.wav',\n",
       "    'source_2_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source2_evaluation_target_mixture.wav'},\n",
       "   '1': {'mixture_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/max/test/mix_both/6930-76324-0027_5683-32879-0011.wav',\n",
       "    'source_1_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/max/test/s1/6930-76324-0027_5683-32879-0011.wav',\n",
       "    'source_2_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/max/test/s2/6930-76324-0027_5683-32879-0011.wav',\n",
       "    'source_1_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source1_evaluation_target_mixture.wav',\n",
       "    'source_2_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source2_evaluation_target_mixture.wav',\n",
       "    'si-snr': 'tensor([[0.1252]])',\n",
       "    'si-sdr': 'tensor([[0.0895]])',\n",
       "    'sdr': 'tensor([[1.0537]])',\n",
       "    'snr': 'tensor([[-7.6336]])',\n",
       "    'pesq': 'tensor([[1.4813]])',\n",
       "    'stoi': 'tensor([[0.6610]])'},\n",
       "   '2': {'mixture_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/max/test/mix_both/1188-133604-0025_4992-23283-0016.wav',\n",
       "    'source_1_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/max/test/s1/1188-133604-0025_4992-23283-0016.wav',\n",
       "    'source_2_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/max/test/s2/1188-133604-0025_4992-23283-0016.wav',\n",
       "    'source_1_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source1_evaluation_target_mixture.wav',\n",
       "    'source_2_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source2_evaluation_target_mixture.wav',\n",
       "    'si-snr': 'tensor([[-20.5538]])',\n",
       "    'si-sdr': 'tensor([[-20.5540]])',\n",
       "    'sdr': 'tensor([[-12.7849]])',\n",
       "    'snr': 'tensor([[-7.3623]])',\n",
       "    'pesq': 'tensor([[1.8410]])',\n",
       "    'stoi': 'tensor([[0.1606]])'}}},\n",
       " 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def speech_separation_evaluate_metric_with_model_on_librimix_2_channels(model, dataset, metrics, n_test, mix_type=\"test_mix_clean_file\"):\n",
    "    task = \"Speech Separation\"\n",
    "    total_si_snr = torch.zeros(0)\n",
    "    total_si_sdr = torch.zeros(0)\n",
    "    total_snr = torch.zeros(0)\n",
    "    total_sdr = torch.zeros(0)\n",
    "    total_nb_pesq = torch.zeros(0)\n",
    "    total_wb_pesq = torch.zeros(0)\n",
    "    total_pit = torch.zeros(0)\n",
    "    total_stoi= torch.zeros(0)\n",
    "    metrics_error = 0 \n",
    "    experiments = {\n",
    "    }\n",
    "\n",
    "    result = {}\n",
    "    # os.walk(dataset_path)\n",
    "    print(\"===================================================>  Dataset: {}\".format(dataset))\n",
    "    # print(Datasets[task])\n",
    "    print(\"Datasets[task][dataset] : {}\".format(Datasets[task][dataset][mix_type]))\n",
    "    test_table = pd.read_table(Datasets[task][dataset][mix_type], sep=\",\")\n",
    "    # display(test_table)\n",
    "    # cols = test_table.iloc[:,1:4]\n",
    "    print(\"len(test_table): {}\".format(len(test_table)))\n",
    "    # display(cols)\n",
    "    for i, row in enumerate(test_table.iterrows()):\n",
    "        preds = torch.zeros(0) \n",
    "        target = torch.zeros(0)\n",
    "        print(\"====> Benchmarking: {}/{}   tot {}\".format(i+1, n_test, test_table.shape[0]))\n",
    "        # print(i, row[1]['mixture_path'], , , row[1]['noise_path'])\n",
    "        mixture_path = row[1]['mixture_path']\n",
    "        source_1_path = row[1]['source_1_path']\n",
    "        source_2_path = row[1]['source_2_path']\n",
    "        #Taking model sample rate\n",
    "        model_sample_rate = AudioAnalysisAPI[model]['sample_rate']\n",
    "        dataset_sample_rate = Datasets[task][dataset]['sample_rate']\n",
    "        print(\"model_sample_rate:{} - dataset_sample_rate:{}\".format(model_sample_rate, dataset_sample_rate))\n",
    "        # Writing on experiment original paths\n",
    "        experiments[str(i)]= {\n",
    "            \"mixture_path\":mixture_path,\n",
    "            \"source_1_path\":source_1_path,\n",
    "            \"source_2_path\":source_2_path,\n",
    "        }\n",
    "        # Loading targets audio on tensors\n",
    "        mixture_waveform, mixture_sample_rate = torchaudio.load(mixture_path)\n",
    "        source_1_waveform, source_1_sample_rate = torchaudio.load(source_1_path)\n",
    "        source_2_waveform, source_2_sample_rate = torchaudio.load(source_2_path)\n",
    "        \n",
    "\n",
    "        if model_sample_rate != dataset_sample_rate:\n",
    "            mixture_waveform = TAF.resample(mixture_waveform, mixture_sample_rate, model_sample_rate)\n",
    "            source_1_waveform = TAF.resample(source_1_waveform, source_1_sample_rate, model_sample_rate)\n",
    "            source_2_waveform = TAF.resample(source_2_waveform, source_2_sample_rate, model_sample_rate)\n",
    "            \n",
    "            mixture_path = \"evaluation_target_mixture.wav\"\n",
    "            source_1_path = \"evaluation_target_source_1.wav\"\n",
    "            source_2_path = \"evaluation_target_source_2.wav\"\n",
    "\n",
    "            torchaudio.save(mixture_path, mixture_waveform, model_sample_rate)\n",
    "            torchaudio.save(source_1_path, source_1_waveform, model_sample_rate)\n",
    "            torchaudio.save(source_2_path, source_2_waveform, model_sample_rate)\n",
    "\n",
    "            if mix_type == \"test_mix_both_file\":\n",
    "                noise_path = row[1]['noise_path']\n",
    "                noise_waveform, noise_sample_rate = torchaudio.load(noise_path)\n",
    "                noise_waveform = TAF.resample(noise_waveform, noise_sample_rate, model_sample_rate)\n",
    "                source_1_waveform = source_1_waveform + noise_waveform\n",
    "                source_2_waveform = source_2_waveform + noise_waveform\n",
    "        else:\n",
    "            if mix_type == \"test_mix_both_file\":\n",
    "                noise_path = row[1]['noise_path']\n",
    "                noise_waveform, noise_sample_rate = torchaudio.load(noise_path)\n",
    "                source_1_waveform = source_1_waveform + noise_waveform\n",
    "                source_2_waveform = source_2_waveform + noise_waveform\n",
    "\n",
    "        # Separate audio files with choosen model\n",
    "        source_1_path_prediction, source_2_path_prediction = AudioAnalysisAPI[model]['function'](audiofile_path=mixture_path)\n",
    "        \n",
    "        # Loading predictions audio on tensors\n",
    "        source_1_prediction_waveform, source_1_prediction_sample_rate = torchaudio.load(source_1_path_prediction)\n",
    "        source_2_prediction_waveform, source_2_prediction_sample_rate = torchaudio.load(source_2_path_prediction)\n",
    "            \n",
    "        # Concatenating predictions into torch tensor \n",
    "        preds = torch.cat((preds, source_1_prediction_waveform), 0)\n",
    "        preds = torch.cat((preds, source_2_prediction_waveform), 0)\n",
    "        # Concatenating targets into torch tensor \n",
    "        target = torch.cat((target, source_1_waveform), 0)\n",
    "        target = torch.cat((target, source_2_waveform), 0)\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "        print(\"mixture_path : {}\".format(mixture_path))\n",
    "        print(\"mixture_sample_rate:{}\".format(mixture_sample_rate))\n",
    "        print(\"mixture_waveform.shape:{}\".format(mixture_waveform.shape))\n",
    "        print(\"mixture_waveform:{}\".format(mixture_waveform))\n",
    "        print()\n",
    "        print(\"source_1_path : {}\".format(source_1_path))\n",
    "        print(\"source_1_sample_rate:{}\".format(source_1_sample_rate))\n",
    "        print(\"source_1_waveform.shape:{}\".format(source_1_waveform.shape))\n",
    "        print(\"source_1_waveform:{}\".format(target[0]))\n",
    "        print()\n",
    "        print(\"source_1_path_prediction : {}\".format(source_1_path_prediction))\n",
    "        print(\"source_1_prediction_sample_rate: {}\".format(source_1_prediction_sample_rate))\n",
    "        print(\"source_1_prediction_waveform.shape : {}\".format(source_1_prediction_waveform.shape))\n",
    "        print(\"source_1_prediction_waveform: {}\".format(preds[0]))\n",
    "        print()\n",
    "        print(\"source_2_path : {}\".format(source_2_path))\n",
    "        print(\"source_2_sample_rate:{}\".format(source_2_sample_rate))\n",
    "        print(\"source_2_waveform.shape:{}\".format(source_2_waveform.shape))\n",
    "        print(\"source_2_waveform:{}\".format(target[1]))\n",
    "        print()\n",
    "        print(\"source_2_path_prediction : {}\".format(source_2_path_prediction))\n",
    "        print(\"source_2_prediction_sample_rate: {}\".format(source_2_prediction_sample_rate))\n",
    "        print(\"source_2_prediction_waveform.shape : {}\".format(source_2_prediction_waveform.shape))\n",
    "        print(\"source_2_prediction_waveform: {}\".format(preds[1]))\n",
    "        print(\"preds.shape {}\\ntarget.shape:{}\".format(preds.shape, target.shape))\n",
    "        print(\"preds: {},\\ntarget:{}\".format(preds, target))\n",
    "        \n",
    "        experiments[str(i)][\"source_1_path_prediction\"] = source_1_path_prediction\n",
    "        experiments[str(i)][\"source_2_path_prediction\"] = source_2_path_prediction\n",
    "        \n",
    "        try:\n",
    "            for metric in metrics:\n",
    "            #print(\"Calculating metric:\", metric)\n",
    "                if metric == \"si-snr\":\n",
    "                    si_snr = ScaleInvariantSignalNoiseRatio()\n",
    "                    si_snr_result = si_snr(preds, target)\n",
    "                    si_snr_result = torch.reshape(si_snr_result, (1, 1))\n",
    "                    total_si_snr = torch.cat((total_si_snr, si_snr_result))\n",
    "                    experiments[str(i)][\"si-snr\"] = str(si_snr_result)\n",
    "\n",
    "                if metric == \"si-sdr\":\n",
    "                    si_sdr = ScaleInvariantSignalDistortionRatio()\n",
    "                    si_sdr_result = si_sdr(preds, target)\n",
    "                    si_sdr_result = torch.reshape(si_sdr_result, (1, 1))\n",
    "                    total_si_sdr = torch.cat((total_si_sdr, si_sdr_result)) \n",
    "                    experiments[str(i)][\"si-sdr\"] = str(si_sdr_result)                \n",
    "\n",
    "                if metric == \"snr\":\n",
    "                    snr = SignalNoiseRatio()\n",
    "                    snr_result = snr(preds, target)\n",
    "                    snr_result = torch.reshape(snr_result, (1, 1))\n",
    "                    total_snr = torch.cat((total_snr, snr_result))\n",
    "                    experiments[str(i)][\"snr\"] = str(snr_result)\n",
    "                    \n",
    "                if metric == \"sdr\":\n",
    "                    sdr = SignalDistortionRatio()\n",
    "                    sdr_result = sdr(preds, target)\n",
    "                    sdr_result = torch.reshape(sdr_result, (1, 1))\n",
    "                    total_sdr = torch.cat((total_sdr, sdr_result))\n",
    "                    experiments[str(i)][\"sdr\"] = str(sdr_result)\n",
    "\n",
    "                if metric == \"pesq\":\n",
    "                    nb_pesq = PerceptualEvaluationSpeechQuality(model_sample_rate, 'nb')\n",
    "                    nb_pesq_result = nb_pesq(preds, target)\n",
    "                    nb_pesq_result = torch.reshape(nb_pesq_result, (1, 1))\n",
    "                    total_nb_pesq = torch.cat((total_nb_pesq, nb_pesq_result))\n",
    "                    experiments[str(i)][\"pesq\"] = str(nb_pesq_result)\n",
    "\n",
    "                    if model_sample_rate > 8000: \n",
    "                        wb_pesq = PerceptualEvaluationSpeechQuality(model_sample_rate, 'wb')\n",
    "                        wb_pesq_result = wb_pesq(preds, target)\n",
    "                        wb_pesq_result = torch.reshape(wb_pesq_result, (1, 1))\n",
    "                        total_wb_pesq = torch.cat((total_wb_pesq, wb_pesq_result))\n",
    "                        experiments[str(i)][\"wb-pesq\"] = str(wb_pesq_result)\n",
    "\n",
    "                if metric == \"pit\":\n",
    "                    pit = PermutationInvariantTraining(signal_distortion_ratio, 'max')\n",
    "                    pit_result = pit(preds, target)\n",
    "                    pit_result = torch.reshape(pit_result, (1, 1))\n",
    "                    total_pit = torch.cat((total_pit, pit_result))\n",
    "                    experiments[str(i)][\"pit\"] = str(pit_result)\n",
    "\n",
    "                if metric == \"stoi\":\n",
    "                    stoi_src = ShortTimeObjectiveIntelligibility(model_sample_rate, False)\n",
    "                    stoi_result = stoi_src(preds, target)\n",
    "                    stoi_result = torch.reshape(stoi_result, (1, 1))\n",
    "                    total_stoi = torch.cat((total_stoi, stoi_result))\n",
    "                    experiments[str(i)][\"stoi\"] = str(stoi_result)\n",
    "        except Exception as e:\n",
    "            print(\"=====> ERROR: {}\".format(e))\n",
    "            metrics_error += 1\n",
    "\n",
    "\n",
    "\n",
    "        if i == n_test - 1: \n",
    "            break\n",
    "\n",
    "    \n",
    "    total_si_snr = torch.sum(total_si_snr)/n_test\n",
    "    total_si_sdr = torch.sum(total_si_sdr)/n_test\n",
    "    total_snr = torch.sum(total_snr)/n_test\n",
    "    total_sdr = torch.sum(total_sdr)/n_test\n",
    "    total_pit = torch.sum(total_pit)/n_test\n",
    "    total_wb_pesq = torch.sum(total_wb_pesq)/n_test\n",
    "    total_nb_pesq = torch.sum(total_nb_pesq)/n_test\n",
    "    total_stoi = torch.sum(total_stoi)/n_test\n",
    "\n",
    "    print(\"============================================================================================\")\n",
    "    print(\"total_si_snr:{}\".format(total_si_snr)) \n",
    "    print(\"total_si_sdr:{}\".format(total_si_sdr)) \n",
    "    print(\"total_snr:{}\".format(total_snr)) \n",
    "    print(\"total_sdr:{}\".format(total_sdr)) \n",
    "    print(\"total_pit:{}\".format(total_pit)) \n",
    "    print(\"total_wb_pesq:{}\".format(total_wb_pesq))  \n",
    "    print(\"total_nb_pesq:{}\".format(total_nb_pesq))  \n",
    "    print(\"total_stoi:{}\".format(total_stoi))  \n",
    "    # Creating the content of result file\n",
    "    result ={\n",
    "        \"model\": model, \n",
    "        \"dataset\": dataset, \n",
    "        \"n_test\": n_test,\n",
    "        \"metrics_error\": metrics_error,\n",
    "        \"model_sample_rate\":model_sample_rate,\n",
    "        \"dataset_sample_rate\":dataset_sample_rate,\n",
    "        \"n_test_done\": n_test - metrics_error,\n",
    "        \"total_si_snr\":str(total_si_snr),\n",
    "        \"total_si_sdr\":str(total_si_sdr),\n",
    "        \"total_snr\":str(total_snr),\n",
    "        \"total_sdr\":str(total_sdr),\n",
    "        \"total_pit\":str(total_pit),\n",
    "        \"total_wb_pesq\":str(total_wb_pesq),\n",
    "        \"total_nb_pesq\":str(total_nb_pesq),\n",
    "        \"total_stoi\":str(total_stoi),\n",
    "        \"experiments\": experiments,\n",
    "    }\n",
    "\n",
    "    # Creating filename\n",
    "    dateTimeObj = datetime.now()\n",
    "    print(dateTimeObj)\n",
    "    timestampStr = dateTimeObj.strftime(\"%d_%b_%Y__%H_%M_%S_%f\")\n",
    "    result_filename = timestampStr + \"_evaluate_\" + model.split(\"/\")[-1] + \"_\" + dataset + \"_\" + mix_type + \"_\" + str(n_test)+\".json\"\n",
    "    print('result_filename : ', result_filename)\n",
    "    with open(result_filename, 'w') as f:\n",
    "        json.dump(result, f)\n",
    "        \n",
    "    return result, n_test\n",
    "\n",
    "speech_separation_2_channels_models = ['/api/audioseparation/speech_separation_sepformer_wsj02mix']\n",
    "speech_separation_dataset = [\"Libri2Mix8kMin\", \"Libri2Mix8kMax\", \"Libri2Mix16kMin\", \"Libri2Mix16kMax\"]\n",
    "metrics = [\"si-snr\", \"si-sdr\", \"sdr\", \"snr\", \"pesq\", \"stoi\"]\n",
    "n_test = 3\n",
    "\n",
    "# type = \"test_mix_clean_file\" #\"test_mix_both_file\" \"test_mix_single_file\"\n",
    "type = \"test_mix_both_file\" \n",
    "# type = \"test_mix_single_file\"\n",
    "\n",
    "# speech_separation_evaluate_metric_with_model_on_librimix_2_channels(\n",
    "#     speech_separation_2_channels_models[0], \n",
    "#     speech_separation_dataset[0],\n",
    "#     [metrics[0], metrics[1], metrics[2], metrics[3], metrics[4], metrics[6]], \n",
    "#     n_test=n_test )\n",
    "speech_separation_evaluate_metric_with_model_on_librimix_2_channels(speech_separation_2_channels_models[0], speech_separation_dataset[3], metrics, n_test=n_test, mix_type=type )\n",
    "# speech_separation_evaluate_metric_with_model_on_librimix_2_channels(speech_separation_2_channels_models[0], speech_separation_dataset[3], metrics, n_test=n_test, mix_type=type )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DLAABE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b806cf2415d9a7125fea09a144cd06efbd6f0375c85c1aa682e3c6e04c90e28f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
