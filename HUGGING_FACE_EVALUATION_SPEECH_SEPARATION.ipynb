{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valeriopuglisi/.conda/envs/DLAABE/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from deep_learning_features_audio import *\n",
    "from deep_learning_dict_api import AudioAnalysisAPI\n",
    "from deep_learning_dict_datasets import Datasets\n",
    "import evaluate\n",
    "from evaluate import load\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "from torchmetrics import ScaleInvariantSignalNoiseRatio, ScaleInvariantSignalDistortionRatio, SignalNoiseRatio,SignalDistortionRatio, PermutationInvariantTraining, \n",
    "from torchmetrics.audio.pesq import PerceptualEvaluationSpeechQuality\n",
    "from torchmetrics.functional.audio import signal_distortion_ratio\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cu102\n",
      "0.11.0+cu102\n"
     ]
    }
   ],
   "source": [
    "# When running this tutorial in Google Colab, install the required packages\n",
    "# with the following.\n",
    "# !pip install torchaudio librosa boto3\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as TAF\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import math\n",
    "import tarfile\n",
    "import multiprocessing\n",
    "\n",
    "import scipy\n",
    "import librosa\n",
    "# import boto3\n",
    "# from botocore import UNSIGNED\n",
    "# from botocore.config import Config\n",
    "import requests\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "[width, height] = matplotlib.rcParams['figure.figsize']\n",
    "if width < 10:\n",
    "  matplotlib.rcParams['figure.figsize'] = [width * 2.5, height]\n",
    "\n",
    "_SAMPLE_DIR = \"_sample_data\"\n",
    "SAMPLE_WAV_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.wav\"\n",
    "SAMPLE_WAV_PATH = os.path.join(_SAMPLE_DIR, \"steam.wav\")\n",
    "\n",
    "SAMPLE_WAV_SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "SAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, \"speech.wav\")\n",
    "\n",
    "SAMPLE_RIR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/room-response/rm1/impulse/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo.wav\"\n",
    "SAMPLE_RIR_PATH = os.path.join(_SAMPLE_DIR, \"rir.wav\")\n",
    "\n",
    "SAMPLE_NOISE_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/distractors/rm1/babb/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\"\n",
    "SAMPLE_NOISE_PATH = os.path.join(_SAMPLE_DIR, \"bg.wav\")\n",
    "\n",
    "SAMPLE_MP3_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.mp3\"\n",
    "SAMPLE_MP3_PATH = os.path.join(_SAMPLE_DIR, \"steam.mp3\")\n",
    "\n",
    "SAMPLE_GSM_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.gsm\"\n",
    "SAMPLE_GSM_PATH = os.path.join(_SAMPLE_DIR, \"steam.gsm\")\n",
    "\n",
    "SAMPLE_TAR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit.tar.gz\"\n",
    "SAMPLE_TAR_PATH = os.path.join(_SAMPLE_DIR, \"sample.tar.gz\")\n",
    "SAMPLE_TAR_ITEM = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "\n",
    "S3_BUCKET = \"pytorch-tutorial-assets\"\n",
    "S3_KEY = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "\n",
    "YESNO_DATASET_PATH = os.path.join(_SAMPLE_DIR, \"yes_no\")\n",
    "os.makedirs(YESNO_DATASET_PATH, exist_ok=True)\n",
    "os.makedirs(_SAMPLE_DIR, exist_ok=True)\n",
    "\n",
    "def _fetch_data():\n",
    "  uri = [\n",
    "    (SAMPLE_WAV_URL, SAMPLE_WAV_PATH),\n",
    "    (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),\n",
    "    (SAMPLE_RIR_URL, SAMPLE_RIR_PATH),\n",
    "    (SAMPLE_NOISE_URL, SAMPLE_NOISE_PATH),\n",
    "    (SAMPLE_MP3_URL, SAMPLE_MP3_PATH),\n",
    "    (SAMPLE_GSM_URL, SAMPLE_GSM_PATH),\n",
    "    (SAMPLE_TAR_URL, SAMPLE_TAR_PATH),\n",
    "  ]\n",
    "  for url, path in uri:\n",
    "    with open(path, 'wb') as file_:\n",
    "      file_.write(requests.get(url).content)\n",
    "\n",
    "_fetch_data()\n",
    "\n",
    "def _download_yesno():\n",
    "  if os.path.exists(os.path.join(YESNO_DATASET_PATH, \"waves_yesno.tar.gz\")):\n",
    "    return\n",
    "  torchaudio.datasets.YESNO(root=YESNO_DATASET_PATH, download=True)\n",
    "\n",
    "YESNO_DOWNLOAD_PROCESS = multiprocessing.Process(target=_download_yesno)\n",
    "YESNO_DOWNLOAD_PROCESS.start()\n",
    "\n",
    "def _get_sample(path, resample=None):\n",
    "  effects = [\n",
    "    [\"remix\", \"1\"]\n",
    "  ]\n",
    "  if resample:\n",
    "    effects.extend([\n",
    "      [\"lowpass\", f\"{resample // 2}\"],\n",
    "      [\"rate\", f'{resample}'],\n",
    "    ])\n",
    "  return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
    "\n",
    "def get_speech_sample(*, resample=None):\n",
    "  return _get_sample(SAMPLE_WAV_SPEECH_PATH, resample=resample)\n",
    "\n",
    "def get_sample(*, resample=None):\n",
    "  return _get_sample(SAMPLE_WAV_PATH, resample=resample)\n",
    "\n",
    "def get_rir_sample(*, resample=None, processed=False):\n",
    "  rir_raw, sample_rate = _get_sample(SAMPLE_RIR_PATH, resample=resample)\n",
    "  if not processed:\n",
    "    return rir_raw, sample_rate\n",
    "  rir = rir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\n",
    "  rir = rir / torch.norm(rir, p=2)\n",
    "  rir = torch.flip(rir, [1])\n",
    "  return rir, sample_rate\n",
    "\n",
    "def get_noise_sample(*, resample=None):\n",
    "  return _get_sample(SAMPLE_NOISE_PATH, resample=resample)\n",
    "\n",
    "def print_stats(waveform, sample_rate=None, src=None):\n",
    "  if src:\n",
    "    print(\"-\" * 10)\n",
    "    print(\"Source:\", src)\n",
    "    print(\"-\" * 10)\n",
    "  if sample_rate:\n",
    "    print(\"Sample Rate:\", sample_rate)\n",
    "  print(\"Shape:\", tuple(waveform.shape))\n",
    "  print(\"Dtype:\", waveform.dtype)\n",
    "  print(f\" - Max:     {waveform.max().item():6.3f}\")\n",
    "  print(f\" - Min:     {waveform.min().item():6.3f}\")\n",
    "  print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n",
    "  print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n",
    "  print()\n",
    "  print(waveform)\n",
    "  print()\n",
    "\n",
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "  figure, axes = plt.subplots(num_channels, 1)\n",
    "  if num_channels == 1:\n",
    "    axes = [axes]\n",
    "  for c in range(num_channels):\n",
    "    axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "    axes[c].grid(True)\n",
    "    if num_channels > 1:\n",
    "      axes[c].set_ylabel(f'Channel {c+1}')\n",
    "    if xlim:\n",
    "      axes[c].set_xlim(xlim)\n",
    "    if ylim:\n",
    "      axes[c].set_ylim(ylim)\n",
    "  figure.suptitle(title)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "  figure, axes = plt.subplots(num_channels, 1)\n",
    "  if num_channels == 1:\n",
    "    axes = [axes]\n",
    "  for c in range(num_channels):\n",
    "    axes[c].specgram(waveform[c], Fs=sample_rate)\n",
    "    if num_channels > 1:\n",
    "      axes[c].set_ylabel(f'Channel {c+1}')\n",
    "    if xlim:\n",
    "      axes[c].set_xlim(xlim)\n",
    "  figure.suptitle(title)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def play_audio(waveform, sample_rate):\n",
    "  waveform = waveform.numpy()\n",
    "\n",
    "  num_channels, num_frames = waveform.shape\n",
    "  if num_channels == 1:\n",
    "    display(Audio(waveform[0], rate=sample_rate))\n",
    "  elif num_channels == 2:\n",
    "    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
    "  else:\n",
    "    raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n",
    "\n",
    "def inspect_file(path):\n",
    "  print(\"-\" * 10)\n",
    "  print(\"Source:\", path)\n",
    "  print(\"-\" * 10)\n",
    "  print(f\" - File size: {os.path.getsize(path)} bytes\")\n",
    "  print(f\" - {torchaudio.info(path)}\")\n",
    "\n",
    "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
    "  fig, axs = plt.subplots(1, 1)\n",
    "  axs.set_title(title or 'Spectrogram (db)')\n",
    "  axs.set_ylabel(ylabel)\n",
    "  axs.set_xlabel('frame')\n",
    "  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
    "  if xmax:\n",
    "    axs.set_xlim((0, xmax))\n",
    "  fig.colorbar(im, ax=axs)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def plot_mel_fbank(fbank, title=None):\n",
    "  fig, axs = plt.subplots(1, 1)\n",
    "  axs.set_title(title or 'Filter bank')\n",
    "  axs.imshow(fbank, aspect='auto')\n",
    "  axs.set_ylabel('frequency bin')\n",
    "  axs.set_xlabel('mel bin')\n",
    "  plt.show(block=False)\n",
    "\n",
    "def get_spectrogram(\n",
    "    n_fft = 400,\n",
    "    win_len = None,\n",
    "    hop_len = None,\n",
    "    power = 2.0,\n",
    "):\n",
    "  waveform, _ = get_speech_sample()\n",
    "  spectrogram = T.Spectrogram(\n",
    "      n_fft=n_fft,\n",
    "      win_length=win_len,\n",
    "      hop_length=hop_len,\n",
    "      center=True,\n",
    "      pad_mode=\"reflect\",\n",
    "      power=power,\n",
    "  )\n",
    "  return spectrogram(waveform)\n",
    "\n",
    "def plot_pitch(waveform, sample_rate, pitch):\n",
    "  figure, axis = plt.subplots(1, 1)\n",
    "  axis.set_title(\"Pitch Feature\")\n",
    "  axis.grid(True)\n",
    "\n",
    "  end_time = waveform.shape[1] / sample_rate\n",
    "  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n",
    "  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n",
    "\n",
    "  axis2 = axis.twinx()\n",
    "  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
    "  ln2 = axis2.plot(\n",
    "      time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n",
    "\n",
    "  axis2.legend(loc=0)\n",
    "  plt.show(block=False)\n",
    "\n",
    "def plot_kaldi_pitch(waveform, sample_rate, pitch, nfcc):\n",
    "  figure, axis = plt.subplots(1, 1)\n",
    "  axis.set_title(\"Kaldi Pitch Feature\")\n",
    "  axis.grid(True)\n",
    "\n",
    "  end_time = waveform.shape[1] / sample_rate\n",
    "  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n",
    "  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n",
    "\n",
    "  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
    "  ln1 = axis.plot(time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n",
    "  axis.set_ylim((-1.3, 1.3))\n",
    "\n",
    "  axis2 = axis.twinx()\n",
    "  time_axis = torch.linspace(0, end_time, nfcc.shape[1])\n",
    "  ln2 = axis2.plot(\n",
    "      time_axis, nfcc[0], linewidth=2, label='NFCC', color='blue', linestyle='--')\n",
    "\n",
    "  lns = ln1 + ln2\n",
    "  labels = [l.get_label() for l in lns]\n",
    "  axis.legend(lns, labels, loc=0)\n",
    "  plt.show(block=False)\n",
    "\n",
    "DEFAULT_OFFSET = 201\n",
    "SWEEP_MAX_SAMPLE_RATE = 48000\n",
    "DEFAULT_LOWPASS_FILTER_WIDTH = 6\n",
    "DEFAULT_ROLLOFF = 0.99\n",
    "DEFAULT_RESAMPLING_METHOD = 'sinc_interpolation'\n",
    "\n",
    "def _get_log_freq(sample_rate, max_sweep_rate, offset):\n",
    "  \"\"\"Get freqs evenly spaced out in log-scale, between [0, max_sweep_rate // 2]\n",
    "\n",
    "  offset is used to avoid negative infinity `log(offset + x)`.\n",
    "\n",
    "  \"\"\"\n",
    "  half = sample_rate // 2\n",
    "  start, stop = math.log(offset), math.log(offset + max_sweep_rate // 2)\n",
    "  return torch.exp(torch.linspace(start, stop, sample_rate, dtype=torch.double)) - offset\n",
    "\n",
    "def _get_inverse_log_freq(freq, sample_rate, offset):\n",
    "  \"\"\"Find the time where the given frequency is given by _get_log_freq\"\"\"\n",
    "  half = sample_rate // 2\n",
    "  return sample_rate * (math.log(1 + freq / offset) / math.log(1 + half / offset))\n",
    "\n",
    "def _get_freq_ticks(sample_rate, offset, f_max):\n",
    "  # Given the original sample rate used for generating the sweep,\n",
    "  # find the x-axis value where the log-scale major frequency values fall in\n",
    "  time, freq = [], []\n",
    "  for exp in range(2, 5):\n",
    "    for v in range(1, 10):\n",
    "      f = v * 10 ** exp\n",
    "      if f < sample_rate // 2:\n",
    "        t = _get_inverse_log_freq(f, sample_rate, offset) / sample_rate\n",
    "        time.append(t)\n",
    "        freq.append(f)\n",
    "  t_max = _get_inverse_log_freq(f_max, sample_rate, offset) / sample_rate\n",
    "  time.append(t_max)\n",
    "  freq.append(f_max)\n",
    "  return time, freq\n",
    "\n",
    "def plot_sweep(waveform, sample_rate, title, max_sweep_rate=SWEEP_MAX_SAMPLE_RATE, offset=DEFAULT_OFFSET):\n",
    "  x_ticks = [100, 500, 1000, 5000, 10000, 20000, max_sweep_rate // 2]\n",
    "  y_ticks = [1000, 5000, 10000, 20000, sample_rate//2]\n",
    "\n",
    "  time, freq = _get_freq_ticks(max_sweep_rate, offset, sample_rate // 2)\n",
    "  freq_x = [f if f in x_ticks and f <= max_sweep_rate // 2 else None for f in freq]\n",
    "  freq_y = [f for f in freq if f >= 1000 and f in y_ticks and f <= sample_rate // 2]\n",
    "\n",
    "  figure, axis = plt.subplots(1, 1)\n",
    "  axis.specgram(waveform[0].numpy(), Fs=sample_rate)\n",
    "  plt.xticks(time, freq_x)\n",
    "  plt.yticks(freq_y, freq_y)\n",
    "  axis.set_xlabel('Original Signal Frequency (Hz, log scale)')\n",
    "  axis.set_ylabel('Waveform Frequency (Hz)')\n",
    "  axis.xaxis.grid(True, alpha=0.67)\n",
    "  axis.yaxis.grid(True, alpha=0.67)\n",
    "  figure.suptitle(f'{title} (sample rate: {sample_rate} Hz)')\n",
    "  plt.show(block=True)\n",
    "\n",
    "def get_sine_sweep(sample_rate, offset=DEFAULT_OFFSET):\n",
    "    max_sweep_rate = sample_rate\n",
    "    freq = _get_log_freq(sample_rate, max_sweep_rate, offset)\n",
    "    delta = 2 * math.pi * freq / sample_rate\n",
    "    cummulative = torch.cumsum(delta, dim=0)\n",
    "    signal = torch.sin(cummulative).unsqueeze(dim=0)\n",
    "    return signal\n",
    "\n",
    "def benchmark_resample(\n",
    "    method,\n",
    "    waveform,\n",
    "    sample_rate,\n",
    "    resample_rate,\n",
    "    lowpass_filter_width=DEFAULT_LOWPASS_FILTER_WIDTH,\n",
    "    rolloff=DEFAULT_ROLLOFF,\n",
    "    resampling_method=DEFAULT_RESAMPLING_METHOD,\n",
    "    beta=None,\n",
    "    librosa_type=None,\n",
    "    iters=5\n",
    "):\n",
    "  if method == \"functional\":\n",
    "    begin = time.time()\n",
    "    for _ in range(iters):\n",
    "      TAF.resample(waveform, sample_rate, resample_rate, lowpass_filter_width=lowpass_filter_width,\n",
    "                 rolloff=rolloff, resampling_method=resampling_method)\n",
    "    elapsed = time.time() - begin\n",
    "    return elapsed / iters\n",
    "  elif method == \"transforms\":\n",
    "    resampler = T.Resample(sample_rate, resample_rate, lowpass_filter_width=lowpass_filter_width,\n",
    "                           rolloff=rolloff, resampling_method=resampling_method, dtype=waveform.dtype)\n",
    "    begin = time.time()\n",
    "    for _ in range(iters):\n",
    "      resampler(waveform)\n",
    "    elapsed = time.time() - begin\n",
    "    return elapsed / iters\n",
    "  elif method == \"librosa\":\n",
    "    waveform_np = waveform.squeeze().numpy()\n",
    "    begin = time.time()\n",
    "    for _ in range(iters):\n",
    "      librosa.resample(waveform_np, sample_rate, resample_rate, res_type=librosa_type)\n",
    "    elapsed = time.time() - begin\n",
    "    return elapsed / iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_separation_evaluate_metric_with_model_on_librimix(model, dataset, metrics, n_test):\n",
    "    task = \"Speech Separation\"\n",
    "    test_mix_both_file = \"test_mix_both_file\"\n",
    "    \n",
    "    tot_si_snr = torch.zeros(0) \n",
    "    tot_si_sdr = torch.zeros(0)\n",
    "    tot_snr = torch.zeros(0)\n",
    "    tot_sdr = torch.zeros(0)\n",
    "    tot_si_pit = torch.zeros(0)\n",
    "    tot_si_pesq = torch.zeros(0)\n",
    "    \n",
    "    references = []\n",
    "    result = {}\n",
    "    # os.walk(dataset_path)\n",
    "    print(\"dataset: {}\".format(dataset))\n",
    "    # print(Datasets[task])\n",
    "    print(\"Datasets[task][dataset] : {}\".format(Datasets[task][dataset][test_mix_both_file]))\n",
    "    test_table = pd.read_table(Datasets[task][dataset][test_mix_both_file], sep=\",\")\n",
    "    # display(test_table)\n",
    "    # cols = test_table.iloc[:,1:4]\n",
    "    print(len(test_table))\n",
    "    # display(cols)\n",
    "    for i, row in enumerate(test_table.iterrows()):\n",
    "        print(\"Benchmarking: {}/{}\".format(i, test_table.shape[0]))\n",
    "        # print(i, row[1]['mixture_path'], , , row[1]['noise_path'])\n",
    "        mixture_path = row[1]['mixture_path']\n",
    "        source_1_path = row[1]['source_1_path']\n",
    "        source_2_path = row[1]['source_2_path']\n",
    "        noise_path = row[1]['noise_path']\n",
    "        source_1_path_prediction, source_2_path_prediction = AudioAnalysisAPI[model]['function'](audiofile_path=mixture_path)\n",
    "        # print(source_1_path_prediction, source_2_path_prediction)\n",
    "\n",
    "        print(\"==> Mixture : {}\".format(mixture_path))\n",
    "        mixture_waveform, mixture_sample_rate = torchaudio.load(mixture_path)\n",
    "        # mixture_path_metadata = torchaudio.info(mixture_path)\n",
    "        # print_stats(mixture_waveform, sample_rate=mixture_sample_rate)\n",
    "        # plot_waveform(mixture_waveform, mixture_sample_rate)\n",
    "        # plot_specgram(mixture_waveform, mixture_sample_rate)\n",
    "        # play_audio(mixture_waveform, mixture_sample_rate)\n",
    "        \n",
    "        print(\"==> source_1 : {}\".format(source_1_path))\n",
    "        source_1_waveform, source_1_sample_rate = torchaudio.load(source_1_path)\n",
    "        # print_stats(source_1_waveform, sample_rate=source_1_sample_rate)\n",
    "        # plot_waveform(source_1_waveform, source_1_sample_rate)\n",
    "        # plot_specgram(source_1_waveform, source_1_sample_rate)\n",
    "        # play_audio(source_1_waveform, source_1_sample_rate)\n",
    "        \n",
    "        print(\"==> source_2 : {}\".format(source_2_path))\n",
    "        source_2_waveform, source_2_sample_rate = torchaudio.load(source_2_path)\n",
    "        # print_stats(source_2_waveform, sample_rate=source_2_sample_rate)\n",
    "        # plot_waveform(source_2_waveform, source_2_sample_rate)\n",
    "        # plot_specgram(source_2_waveform, source_2_sample_rate)\n",
    "        # play_audio(source_2_waveform, source_2_sample_rate)\n",
    "        \n",
    "        print(\"==> source_1_prediction : {}\".format(source_1_path_prediction))\n",
    "        source_1_prediction_waveform, source_1_prediction_sample_rate = torchaudio.load(source_1_path_prediction)\n",
    "        # print_stats(source_1_prediction_waveform, sample_rate=source_1_prediction_sample_rate)\n",
    "        # plot_waveform(source_1_prediction_waveform, source_1_prediction_sample_rate)\n",
    "        # plot_specgram(source_1_prediction_waveform, source_1_prediction_sample_rate)\n",
    "        # play_audio(source_1_prediction_waveform, source_1_prediction_sample_rate)\n",
    "        \n",
    "        print(\"==> source_2_prediction : {}\".format(source_2_path_prediction))\n",
    "        source_2_prediction_waveform, source_2_prediction_sample_rate = torchaudio.load(source_2_path_prediction)\n",
    "        # print_stats(source_2_prediction_waveform, sample_rate=source_2_prediction_sample_rate)\n",
    "        # plot_waveform(source_2_prediction_waveform, source_2_prediction_sample_rate)\n",
    "        # plot_specgram(source_2_prediction_waveform, source_2_prediction_sample_rate)\n",
    "        # play_audio(source_2_prediction_waveform, source_2_prediction_sample_rate)\n",
    "\n",
    "        resample_rate_src_1 = max(source_1_sample_rate, source_1_prediction_sample_rate)\n",
    "        resample_rate_src_2 = max(source_2_sample_rate, source_2_prediction_sample_rate)\n",
    "        print(resample_rate_src_1, resample_rate_src_2)\n",
    "        \n",
    "        # resampling to maximim frequency because is not sure that a model works with the original frequency\n",
    "        source_1_waveform = TAF.resample(source_1_waveform, source_1_sample_rate, resample_rate_src_1)\n",
    "        source_2_waveform = TAF.resample(source_2_waveform, source_2_sample_rate, resample_rate_src_2)\n",
    "        source_1_prediction_waveform = TAF.resample(source_1_prediction_waveform, source_1_prediction_sample_rate, resample_rate_src_1)\n",
    "        source_2_prediction_waveform = TAF.resample(source_2_prediction_waveform, source_2_prediction_sample_rate, resample_rate_src_2)\n",
    "\n",
    "        for metric in metrics:\n",
    "            print(\"Calculating metric:\", metric)\n",
    "            if metric == \"si-snr\":\n",
    "                si_snr = ScaleInvariantSignalNoiseRatio()\n",
    "                source_1_si_snr = si_snr(source_1_prediction_waveform, source_1_waveform)\n",
    "                source_2_si_snr = si_snr(source_2_prediction_waveform, source_2_waveform)\n",
    "                print(\"{}: {}\".format(metric, (source_1_si_snr,source_2_si_snr)))\n",
    "                result[str(i)+'_'+metric] = {\n",
    "                    \"metric\" : metric,\n",
    "                    \"mixture_path\": mixture_path,\n",
    "                    \"source_1_path\": source_1_path,\n",
    "                    \"source_2_path\": source_2_path,\n",
    "                    \"source_1_path_prediction\": source_1_path_prediction,\n",
    "                    \"source_2_path_prediction\":source_2_path_prediction,\n",
    "                    \"noise_path\": noise_path,\n",
    "                    \"source_1_si-snr\": str(source_1_si_snr),\n",
    "                    \"source_2_si-snr\" : str(source_2_si_snr),\n",
    "                        \n",
    "                    }\n",
    "            if metric == \"si-sdr\":\n",
    "                si_sdr = ScaleInvariantSignalDistortionRatio()\n",
    "                source_1_si_sdr = si_sdr(source_1_prediction_waveform, source_1_waveform)\n",
    "                source_2_si_sdr = si_sdr(source_2_prediction_waveform, source_2_waveform)\n",
    "                print(\"{}: {}\".format(metric, (source_1_si_sdr,source_2_si_sdr)))\n",
    "                result[str(i)+'_'+metric] = {\n",
    "                    \"metric\" : metric,\n",
    "                    \"mixture_path\": mixture_path,\n",
    "                    \"source_1_path\": source_1_path,\n",
    "                    \"source_2_path\": source_2_path,\n",
    "                    \"source_1_path_prediction\": source_1_path_prediction,\n",
    "                    \"source_2_path_prediction\":source_2_path_prediction,\n",
    "                    \"noise_path\": noise_path,\n",
    "                    \"source_1_si-snr\": str(source_1_si_sdr),\n",
    "                    \"source_2_si-snr\" : str(source_2_si_sdr),        \n",
    "                }\n",
    "                                \n",
    "\n",
    "            if metric == \"snr\":\n",
    "                snr = SignalNoiseRatio()\n",
    "                source_1_snr = snr(source_1_prediction_waveform, source_1_waveform)\n",
    "                source_2_snr = snr(source_2_prediction_waveform, source_2_waveform)\n",
    "                print(\"{}: {}\".format(metric, (source_1_snr,source_2_snr)))\n",
    "                result[str(i)+'_'+metric] = {\n",
    "                    \"metric\" : metric,\n",
    "                    \"mixture_path\": mixture_path,\n",
    "                    \"source_1_path\": source_1_path,\n",
    "                    \"source_2_path\": source_2_path,\n",
    "                    \"source_1_path_prediction\": source_1_path_prediction,\n",
    "                    \"source_2_path_prediction\":source_2_path_prediction,\n",
    "                    \"noise_path\": noise_path,\n",
    "                    \"source_1_snr\": str(source_1_snr),\n",
    "                    \"source_2_snr\" : str(source_2_snr),\n",
    "                    \n",
    "                }\n",
    "            if metric == \"sdr\":\n",
    "                sdr = SignalDistortionRatio()\n",
    "                source_1_sdr = sdr(source_1_prediction_waveform, source_1_waveform)\n",
    "                source_2_sdr = sdr(source_2_prediction_waveform, source_2_waveform)\n",
    "                print(\"{}: {}\".format(metric, (source_1_sdr,source_2_sdr)))\n",
    "                result[str(i)+'_'+metric] = {\n",
    "                    \"metric\" : metric,\n",
    "                    \"mixture_path\": mixture_path,\n",
    "                    \"source_1_path\": source_1_path,\n",
    "                    \"source_2_path\": source_2_path,\n",
    "                    \"source_1_path_prediction\": source_1_path_prediction,\n",
    "                    \"source_2_path_prediction\":source_2_path_prediction,\n",
    "                    \"noise_path\": noise_path,\n",
    "                    \"source_1_sdr\": str(source_1_sdr),\n",
    "                    \"source_2_sdr\" : str(source_2_sdr),\n",
    "                    \n",
    "                }\n",
    "            if metric == \"pesq\":\n",
    "                nb_pesq = PerceptualEvaluationSpeechQuality(resample_rate_src_1, 'nb')\n",
    "                wb_pesq = PerceptualEvaluationSpeechQuality(resample_rate_src_1, 'wb')\n",
    "                source_1_nb_pesq = nb_pesq(source_1_prediction_waveform, source_1_waveform)\n",
    "                source_2_nb_pesq = nb_pesq(source_2_prediction_waveform, source_2_waveform)\n",
    "\n",
    "                source_1_wb_pesq = wb_pesq(source_1_prediction_waveform, source_1_waveform)\n",
    "                source_2_wb_pesq = wb_pesq(source_2_prediction_waveform, source_2_waveform)\n",
    "                print(\"{}: {}\".format(metric, (source_1_wb_pesq,source_2_wb_pesq)))\n",
    "                result[str(i)+'_'+metric] = {\n",
    "                    \"metric\" : metric,\n",
    "                    \"mixture_path\": mixture_path,\n",
    "                    \"source_1_path\": source_1_path,\n",
    "                    \"source_2_path\": source_2_path,\n",
    "                    \"source_1_path_prediction\": source_1_path_prediction,\n",
    "                    \"source_2_path_prediction\":source_2_path_prediction,\n",
    "                    \"noise_path\": noise_path,\n",
    "                    \"source_1_nb_pesq\": str(source_1_nb_pesq),\n",
    "                    \"source_2_nb_pesq\" : str(source_2_nb_pesq),\n",
    "                    \"source_1_wb_pesq\": str(source_1_wb_pesq),\n",
    "                    \"source_2_wb_pesq\" : str(source_2_wb_pesq),\n",
    "                }   \n",
    "\n",
    "\n",
    "\n",
    "            if metric == \"pit\":\n",
    "                pit = PermutationInvariantTraining(signal_distortion_ratio, 'max')\n",
    "                source_1_pit = pit(source_1_prediction_waveform, source_1_waveform)\n",
    "                source_2_pit = pit(source_2_prediction_waveform, source_2_waveform)\n",
    "                print(\"{}: {}\".format(metric, (source_1_pit,source_2_pit)))\n",
    "                result[str(i)+'_'+metric] = {\n",
    "                    \"metric\" : metric,\n",
    "                    \"mixture_path\": mixture_path,\n",
    "                    \"source_1_path\": source_1_path,\n",
    "                    \"source_2_path\": source_2_path,\n",
    "                    \"source_1_path_prediction\": source_1_path_prediction,\n",
    "                    \"source_2_path_prediction\":source_2_path_prediction,\n",
    "                    \"noise_path\": noise_path,\n",
    "                    \"source_1_pit\": str(source_1_pit),\n",
    "                    \"source_2_pit\" : str(source_2_pit),\n",
    "                    \n",
    "                }\n",
    "                pass\n",
    "\n",
    "            \n",
    "        \n",
    "        if i == n_test - 1: \n",
    "            break\n",
    "\n",
    "       \n",
    "\n",
    "    params = {\"model\": model, \"dataset\": dataset, \"n_test\": n_test}\n",
    "    evaluate.save(path_or_file=\"./results/\", **result, **params)\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: Libri2Mix16kMin\n",
      "Datasets[task][dataset] : /storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/min/metadata/mixture_test_mix_both.csv\n",
      "3000\n",
      "Benchmarking: 0/3000\n",
      "Resampling the audio from 16000 Hz to 8000 Hz\n",
      "==> Mixture : /storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/min/test/mix_both/4077-13754-0001_5142-33396-0065.wav\n",
      "==> source_1 : /storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/min/test/s1/4077-13754-0001_5142-33396-0065.wav\n",
      "==> source_2 : /storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/min/test/s2/4077-13754-0001_5142-33396-0065.wav\n",
      "==> source_1_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source1_4077-13754-0001_5142-33396-0065.wav\n",
      "==> source_2_prediction : /storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source2_4077-13754-0001_5142-33396-0065.wav\n",
      "16000 16000\n",
      "Calculating metric: si-snr\n",
      "si-snr: (tensor(7.4130), tensor(9.4408))\n",
      "Calculating metric: si-sdr\n",
      "Calculating metric: pesq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valeriopuglisi/.conda/envs/DLAABE/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (ScaleInvariantSignalNoiseRatio). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pesq: (tensor(1.4443), tensor(1.6474))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0_si-snr': {'metric': 'si-snr',\n",
       "  'mixture_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/min/test/mix_both/4077-13754-0001_5142-33396-0065.wav',\n",
       "  'source_1_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/min/test/s1/4077-13754-0001_5142-33396-0065.wav',\n",
       "  'source_2_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/min/test/s2/4077-13754-0001_5142-33396-0065.wav',\n",
       "  'source_1_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source1_4077-13754-0001_5142-33396-0065.wav',\n",
       "  'source_2_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source2_4077-13754-0001_5142-33396-0065.wav',\n",
       "  'noise_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/min/test/noise/4077-13754-0001_5142-33396-0065.wav',\n",
       "  'source_1_si-snr': 'tensor(7.4130)',\n",
       "  'source_2_si-snr': 'tensor(9.4408)'},\n",
       " '0_pesq': {'metric': 'pesq',\n",
       "  'mixture_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/min/test/mix_both/4077-13754-0001_5142-33396-0065.wav',\n",
       "  'source_1_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/min/test/s1/4077-13754-0001_5142-33396-0065.wav',\n",
       "  'source_2_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/min/test/s2/4077-13754-0001_5142-33396-0065.wav',\n",
       "  'source_1_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source1_4077-13754-0001_5142-33396-0065.wav',\n",
       "  'source_2_path_prediction': '/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/separation_sepformer_wsj02mix/SpeechSeparationSepformerWsj02mix_source2_4077-13754-0001_5142-33396-0065.wav',\n",
       "  'noise_path': '/storage/data_8T/datasets/audio/LibriMix/Libri2Mix/wav16k/min/test/noise/4077-13754-0001_5142-33396-0065.wav',\n",
       "  'source_1_nb_pesq': 'tensor(2.0254)',\n",
       "  'source_2_nb_pesq': 'tensor(2.5565)',\n",
       "  'source_1_wb_pesq': 'tensor(1.4443)',\n",
       "  'source_2_wb_pesq': 'tensor(1.6474)'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_separation_models = [\n",
    "    '/api/audioseparation/speech_separation_sepformer_wsj02mix',\n",
    "    '/api/audioseparation/speech_separation_sepformer_wsj03mix'\n",
    "    ]\n",
    "speech_separation_dataset = \"Libri2Mix16kMin\"\n",
    "metrics = [\"si-snr\", \"si-sdr\", \"pesq\"]\n",
    "\n",
    "speech_separation_evaluate_metric_with_model_on_librimix(speech_separation_models[0], speech_separation_dataset, metrics, 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DLAABE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b806cf2415d9a7125fea09a144cd06efbd6f0375c85c1aa682e3c6e04c90e28f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
