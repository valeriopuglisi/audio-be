{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valeriopuglisi/.conda/envs/DeepLearningAudioAnalyzer/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Reusing dataset librispeech_asr_demo (/home/valeriopuglisi/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_demo/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n",
      "Some weights of the model checkpoint at facebook/data2vec-audio-base-960h were not used when initializing Data2VecAudioModel: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Data2VecAudioModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Data2VecAudioModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 292, 768]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Data2VecAudioModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n",
    "model = Data2VecAudioModel.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n",
    "\n",
    "# audio file is decoded on the fly\n",
    "inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.9335,  0.3089, -0.6611,  ..., -0.1616,  0.2516,  0.3117],\n",
      "         [-0.9335,  0.3089, -0.6611,  ..., -0.1616,  0.2516,  0.3117],\n",
      "         [-0.9334,  0.3089, -0.6611,  ..., -0.1616,  0.2516,  0.3117],\n",
      "         ...,\n",
      "         [-1.0996, -0.1713, -0.2078,  ..., -0.2466, -0.0577, -0.0303],\n",
      "         [-1.1000, -0.1701, -0.2100,  ..., -0.2475, -0.0610, -0.0314],\n",
      "         [-1.1015, -0.1651, -0.2113,  ..., -0.2483, -0.0623, -0.0311]]])\n"
     ]
    }
   ],
   "source": [
    "last_hidden_states.shape\n",
    "print(last_hidden_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr_demo (/home/valeriopuglisi/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_demo/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Data2VecAudioForAudioFrameClassification\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"hf-internal-testing/tiny-random-data2vec-audio-frame\")\n",
    "model = Data2VecAudioForAudioFrameClassification.from_pretrained(\"hf-internal-testing/tiny-random-data2vec-audio-frame\")\n",
    "\n",
    "# audio file is decoded on the fly\n",
    "inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], return_tensors=\"pt\", sampling_rate=sampling_rate)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "probabilities = torch.sigmoid(logits[0])\n",
    "# labels is a one-hot array of shape (num_frames, num_speakers)\n",
    "labels = (probabilities > 0.5).long()\n",
    "labels[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA2VEC FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr_demo (/home/valeriopuglisi/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_demo/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speakers are the same!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Data2VecAudioForXVector\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"hf-internal-testing/tiny-random-data2vec-xvector\")\n",
    "model = Data2VecAudioForXVector.from_pretrained(\"hf-internal-testing/tiny-random-data2vec-xvector\")\n",
    "\n",
    "# audio file is decoded on the fly\n",
    "audio_files = [d[\"array\"] for d in dataset[:2][\"audio\"]]\n",
    "\n",
    "\n",
    "inputs = feature_extractor(audio_files, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs).embeddings\n",
    "\n",
    "embeddings = torch.nn.functional.normalize(embeddings, dim=-1).cpu()\n",
    "\n",
    "# the resulting embeddings can be used for cosine similarity-based retrieval\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "similarity = cosine_sim(embeddings[0], embeddings[1])\n",
    "threshold = 0.7  # the optimal threshold is dataset-dependent\n",
    "if similarity < threshold:\n",
    "    print(\"Speakers are not the same!\")\n",
    "else:\n",
    "    print(\"Speakers are the same!\")\n",
    "round(similarity.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr_demo (/home/valeriopuglisi/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_demo/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n",
      "/home/valeriopuglisi/.conda/envs/DeepLearningAudioAnalyzer/lib/python3.10/site-packages/transformers/utils/generic.py:136: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return np.array(obj)\n",
      "/home/valeriopuglisi/.conda/envs/DeepLearningAudioAnalyzer/lib/python3.10/site-packages/transformers/utils/generic.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(obj)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/DeepLearningAudioAnalyzer/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:168\u001b[0m, in \u001b[0;36mBatchFeature.convert_to_tensors\u001b[0;34m(self, tensor_type)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 168\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    170\u001b[0m     \u001b[39mself\u001b[39m[key] \u001b[39m=\u001b[39m tensor\n",
      "File \u001b[0;32m~/.conda/envs/DeepLearningAudioAnalyzer/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:150\u001b[0m, in \u001b[0;36mBatchFeature.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    149\u001b[0m     value \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(value)\n\u001b[0;32m--> 150\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensor(value)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m feature_extractor \u001b[39m=\u001b[39m Wav2Vec2FeatureExtractor\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mhf-internal-testing/tiny-random-data2vec-xvector\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m model \u001b[39m=\u001b[39m Data2VecAudioForXVector\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mhf-internal-testing/tiny-random-data2vec-xvector\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m inputs \u001b[39m=\u001b[39m feature_extractor(audio_files, sampling_rate\u001b[39m=\u001b[39;49msampling_rate, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     25\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     26\u001b[0m     embeddings \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\u001b[39m.\u001b[39membeddings\n",
      "File \u001b[0;32m~/.conda/envs/DeepLearningAudioAnalyzer/lib/python3.10/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py:196\u001b[0m, in \u001b[0;36mWav2Vec2FeatureExtractor.__call__\u001b[0;34m(self, raw_speech, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors, sampling_rate, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39m# convert into correct format for padding\u001b[39;00m\n\u001b[1;32m    194\u001b[0m encoded_inputs \u001b[39m=\u001b[39m BatchFeature({\u001b[39m\"\u001b[39m\u001b[39minput_values\u001b[39m\u001b[39m\"\u001b[39m: raw_speech})\n\u001b[0;32m--> 196\u001b[0m padded_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad(\n\u001b[1;32m    197\u001b[0m     encoded_inputs,\n\u001b[1;32m    198\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m    199\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    200\u001b[0m     truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m    201\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    202\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    203\u001b[0m )\n\u001b[1;32m    205\u001b[0m \u001b[39m# convert input values to correct format\u001b[39;00m\n\u001b[1;32m    206\u001b[0m input_values \u001b[39m=\u001b[39m padded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_values\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/DeepLearningAudioAnalyzer/lib/python3.10/site-packages/transformers/feature_extraction_sequence_utils.py:225\u001b[0m, in \u001b[0;36mSequenceFeatureExtractor.pad\u001b[0;34m(self, processed_features, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors)\u001b[0m\n\u001b[1;32m    222\u001b[0m             value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    223\u001b[0m         batch_outputs[key]\u001b[39m.\u001b[39mappend(value)\n\u001b[0;32m--> 225\u001b[0m \u001b[39mreturn\u001b[39;00m BatchFeature(batch_outputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n",
      "File \u001b[0;32m~/.conda/envs/DeepLearningAudioAnalyzer/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:79\u001b[0m, in \u001b[0;36mBatchFeature.__init__\u001b[0;34m(self, data, tensor_type)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, tensor_type: Union[\u001b[39mNone\u001b[39;00m, \u001b[39mstr\u001b[39m, TensorType] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     78\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(data)\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type)\n",
      "File \u001b[0;32m~/.conda/envs/DeepLearningAudioAnalyzer/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:174\u001b[0m, in \u001b[0;36mBatchFeature.convert_to_tensors\u001b[0;34m(self, tensor_type)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_values\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    173\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing values of different lengths. \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 174\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    175\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate padding \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwith \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m         )\n\u001b[1;32m    179\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Data2VecAudioForXVector\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as TAF\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "dataset = dataset.sort(\"id\")\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "\n",
    "\n",
    "source_1_waveform_path = \"/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/common_voice_it_17415780.wav\"\n",
    "source_2_waveform_path= \"/storage/data_itcoin/svoice_space/DeepLearningAudioAnalyzer/audio-be/common_voice_en_1047.wav\"\n",
    "source_1_waveform, sample_rate_1 = torchaudio.load(source_1_waveform_path)\n",
    "source_2_waveform, sample_rate_2 = torchaudio.load(source_2_waveform_path)\n",
    "resample_rate = 16000\n",
    "source_1_waveform = TAF.resample(source_1_waveform, sample_rate_1, resample_rate)\n",
    "source_2_waveform = TAF.resample(source_2_waveform, sample_rate_1, resample_rate)\n",
    "audio_files=[source_1_waveform, source_2_waveform]\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"hf-internal-testing/tiny-random-data2vec-xvector\")\n",
    "model = Data2VecAudioForXVector.from_pretrained(\"hf-internal-testing/tiny-random-data2vec-xvector\")\n",
    "inputs = feature_extractor(audio_files, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs).embeddings\n",
    "embeddings = torch.nn.functional.normalize(embeddings, dim=-1).cpu()\n",
    "\n",
    "# the resulting embeddings can be used for cosine similarity-based retrieval\n",
    "cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "similarity = cosine_sim(embeddings[0], embeddings[1])\n",
    "threshold = 0.7  # the optimal threshold is dataset-dependent\n",
    "\n",
    "if similarity < threshold:\n",
    "    print(\"Speakers are not the same!\")\n",
    "else:\n",
    "    print(\"Speakers are the same!\")\n",
    "round(similarity.item(), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DeepLearningAudioAnalyzer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac535e852edeaa4a77d739a49a3f8ff5a978d18bbd365140b2269f4dc9ac2f09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
