{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_learning_features_audio import *\n",
    "from deep_learning_dict_api import AudioAnalysisAPI\n",
    "from deep_learning_dict_datasets import Datasets\n",
    "import evaluate\n",
    "from evaluate import load\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asr_evaluate_metric_with_model_on_commonvoice(task, dataset, model, metrics, n_test):\n",
    "    test_done = 0\n",
    "    errors = 0\n",
    "    predictions = []\n",
    "    references = []\n",
    "    result = {\n",
    "        \"evaluation\": {}\n",
    "    }\n",
    "    # os.walk(dataset_path)\n",
    "    test_table = pd.read_table(Datasets[task][dataset][\"test_file\"])\n",
    "    test_audio_path = Datasets[task][dataset][\"path\"]\n",
    "    tot_sample = test_table.shape[0]\n",
    "\n",
    "    for i, row in enumerate(test_table.iterrows()):\n",
    "        print(\"Benchmarking: {}/{}\".format(i, test_table.shape[0]))\n",
    "        audiofile_path = row[1][\"path\"]\n",
    "        wav_audiofile_path = os.path.splitext(audiofile_path)[0] + '.wav'\n",
    "        reference = row[1][\"sentence\"]\n",
    "        print(\"reference:{}\".format(reference))\n",
    "        try:\n",
    "            audio_path = os.path.join(test_audio_path, \"wavs\", wav_audiofile_path)\n",
    "            if os.path.isfile(audio_path):\n",
    "                prediction = AudioAnalysisAPI[model]['function'](audiofile_path=audio_path)\n",
    "                if prediction is not None and prediction != \"\":\n",
    "                    predictions.append(prediction.lower())\n",
    "                    references.append(reference.lower())\n",
    "                    print(\"audiofile_path: {}\".format(audio_path))\n",
    "                    print(\"reference: {}\".format(reference.lower()))\n",
    "                    print(\"prediction:{}\\n\".format(prediction.lower()))\n",
    "                    test_done += 1\n",
    "            else:\n",
    "                print(audio_path, \" file doesn't exist\")\n",
    "                errors+=1\n",
    "            if i == n_test:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            errors += 1\n",
    "            pass\n",
    "    for metric in metrics:\n",
    "        loaded_metric = load(metric)\n",
    "        # wer = load(\"wer\")\n",
    "        caluculated_metric = loaded_metric.compute(predictions=predictions, references=references)\n",
    "        # wer_score = wer.compute(predictions=predictions, references=references)\n",
    "        print(\"{}: {}\".format(metric, caluculated_metric))\n",
    "        # print(\"wer_score: {}\".format(wer_score))\n",
    "        result['evaluation'][metric] = caluculated_metric\n",
    "    params = {\"model\": model,\n",
    "            \"dataset\": dataset,\n",
    "            \"n_test\": n_test,\n",
    "            \"test_done\": test_done,\n",
    "            \"errors\": errors,\n",
    "            \"tot_sample\": tot_sample,\n",
    "\n",
    "            }\n",
    "    evaluate.save(path_or_file=\"./results/\", **result, **params)\n",
    "    return result    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Benchmark of model: asr_wav2vec2_voxpopuli_es -- dataset: CommonVoice-ES-10.0 \n",
      "Benchmarking: 0/15459\n",
      "reference:Habita en aguas poco profundas y rocosas.\n",
      "Waveform shape: torch.Size([1, 176256]) - Sample Rate: 48000\n",
      "Sample Rate: 16000\n",
      "Labels: ('-', '|', 'e', 'a', 'o', 's', 'n', 'r', 'i', 'l', 'd', 'c', 't', 'u', 'p', 'm', 'b', 'q', 'y', 'g', 'v', 'h', 'ó', 'f', 'í', 'á', 'j', 'z', 'ñ', 'é', 'x', 'ú', 'k', 'w', 'ü')\n",
      "Class labels: ('-', '|', 'e', 'a', 'o', 's', 'n', 'r', 'i', 'l', 'd', 'c', 't', 'u', 'p', 'm', 'b', 'q', 'y', 'g', 'v', 'h', 'ó', 'f', 'í', 'á', 'j', 'z', 'ñ', 'é', 'x', 'ú', 'k', 'w', 'ü')\n",
      "audiofile_path: /storage/data_8T/datasets/audio/common-voice-corpus-10.0-2022-07-04/es/wavs/common_voice_es_19698530.wav\n",
      "reference: habita en aguas poco profundas y rocosas.\n",
      "prediction:abintaen aguas poco profinas quedo costes\n",
      "\n",
      "Benchmarking: 1/15459\n",
      "reference:Opera principalmente vuelos de cabotaje y regionales de carga.\n",
      "Waveform shape: torch.Size([1, 329472]) - Sample Rate: 48000\n",
      "Sample Rate: 16000\n",
      "Labels: ('-', '|', 'e', 'a', 'o', 's', 'n', 'r', 'i', 'l', 'd', 'c', 't', 'u', 'p', 'm', 'b', 'q', 'y', 'g', 'v', 'h', 'ó', 'f', 'í', 'á', 'j', 'z', 'ñ', 'é', 'x', 'ú', 'k', 'w', 'ü')\n",
      "Class labels: ('-', '|', 'e', 'a', 'o', 's', 'n', 'r', 'i', 'l', 'd', 'c', 't', 'u', 'p', 'm', 'b', 'q', 'y', 'g', 'v', 'h', 'ó', 'f', 'í', 'á', 'j', 'z', 'ñ', 'é', 'x', 'ú', 'k', 'w', 'ü')\n",
      "audiofile_path: /storage/data_8T/datasets/audio/common-voice-corpus-10.0-2022-07-04/es/wavs/common_voice_es_19987333.wav\n",
      "reference: opera principalmente vuelos de cabotaje y regionales de carga.\n",
      "prediction:coopera principalmente huelo de carbotajes y regionales de carga \n",
      "\n",
      "Benchmarking: 2/15459\n",
      "reference:Para visitar contactar primero con la dirección.\n",
      "Waveform shape: torch.Size([1, 319104]) - Sample Rate: 48000\n",
      "Sample Rate: 16000\n",
      "Labels: ('-', '|', 'e', 'a', 'o', 's', 'n', 'r', 'i', 'l', 'd', 'c', 't', 'u', 'p', 'm', 'b', 'q', 'y', 'g', 'v', 'h', 'ó', 'f', 'í', 'á', 'j', 'z', 'ñ', 'é', 'x', 'ú', 'k', 'w', 'ü')\n",
      "Class labels: ('-', '|', 'e', 'a', 'o', 's', 'n', 'r', 'i', 'l', 'd', 'c', 't', 'u', 'p', 'm', 'b', 'q', 'y', 'g', 'v', 'h', 'ó', 'f', 'í', 'á', 'j', 'z', 'ñ', 'é', 'x', 'ú', 'k', 'w', 'ü')\n",
      "audiofile_path: /storage/data_8T/datasets/audio/common-voice-corpus-10.0-2022-07-04/es/wavs/common_voice_es_19691402.wav\n",
      "reference: para visitar contactar primero con la dirección.\n",
      "prediction:para visitar contactar primero con la dirección \n",
      "\n",
      "Benchmarking: 3/15459\n",
      "reference:En los dos años siguientes trabajó de manera constante, aunque en papeles menores.\n",
      "Waveform shape: torch.Size([1, 230400]) - Sample Rate: 44100\n",
      "Sample Rate: 16000\n",
      "Labels: ('-', '|', 'e', 'a', 'o', 's', 'n', 'r', 'i', 'l', 'd', 'c', 't', 'u', 'p', 'm', 'b', 'q', 'y', 'g', 'v', 'h', 'ó', 'f', 'í', 'á', 'j', 'z', 'ñ', 'é', 'x', 'ú', 'k', 'w', 'ü')\n",
      "Class labels: ('-', '|', 'e', 'a', 'o', 's', 'n', 'r', 'i', 'l', 'd', 'c', 't', 'u', 'p', 'm', 'b', 'q', 'y', 'g', 'v', 'h', 'ó', 'f', 'í', 'á', 'j', 'z', 'ñ', 'é', 'x', 'ú', 'k', 'w', 'ü')\n",
      "audiofile_path: /storage/data_8T/datasets/audio/common-voice-corpus-10.0-2022-07-04/es/wavs/common_voice_es_23747242.wav\n",
      "reference: en los dos años siguientes trabajó de manera constante, aunque en papeles menores.\n",
      "prediction:en los dos años siguientes trabajó de manera constante aunque en papeles menores \n",
      "\n",
      "wer: 0.3333333333333333\n",
      "cer: 0.09871244635193133\n"
     ]
    }
   ],
   "source": [
    "task = \"Automatic Speech Recognition\"\n",
    "dataset = \"CommonVoice-FR-10.0\"\n",
    "models = [\n",
    "    '/api/automatic_speech_recognition/asr_wav2vec2_commonvoice_fr',\n",
    "    '/api/automatic_speech_recognition/asr_wav2vec2_voxpopuli_fr',\n",
    "    '/api/automatic_speech_recognition/asr_crdnn_commonvoice_fr',\n",
    "    ]\n",
    "metrics = [\"wer\", \"cer\"]\n",
    "\n",
    "n_test = 3\n",
    "\n",
    "for model in models:\n",
    "    print(\"===== Benchmark of model: {} -- dataset: {} \".format(model.split(\"/\")[-1], dataset))\n",
    "    asr_evaluate_metric_with_model_on_commonvoice(\n",
    "        task=task,\n",
    "        dataset=dataset,\n",
    "        model=model,\n",
    "        metrics=metrics,\n",
    "        n_test=n_test\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DLAABE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b806cf2415d9a7125fea09a144cd06efbd6f0375c85c1aa682e3c6e04c90e28f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
